

# **Architecting a High-Fidelity Geospatial Intelligence Pipeline for Social Media Event Extraction**

## **Section 1: Analysis of Current Data Output and Inferred Pipeline Deficiencies**

A thorough analysis of the provided data output, map\_data.csv, reveals critical flaws that undermine the core objective of the mapping application. These issues are not isolated bugs but are symptomatic of systemic deficiencies in the underlying data processing pipeline. The current system fails to correctly interpret location, time, and event context, resulting in data that is not only inaccurate but actively misleading. This section provides a detailed diagnosis of these flaws and infers their root causes within the existing technical architecture.

### **1.1 Critical Flaw Analysis of map\_data.csv**

The output data exhibits several categories of severe errors, demonstrating a fundamental failure in data processing logic across multiple stages.

#### **Geocoding Context Failure**

The most significant error is the pipeline's inability to use contextual information during geocoding. A data point titled "Sighting near Armitage" is assigned the coordinates . This location is in Armitage, Staffordshire, United Kingdom. However, the data's SourceURL points to a post on the r/chicago subreddit, and the text snippet explicitly mentions "Central Park and Armitage," a well-known intersection in Chicago, Illinois.1 This indicates a naive geocoding process that extracts the first plausible toponym ("Armitage") and queries a geocoding service without providing any disambiguating context, such as the city or state. The failure to leverage the most obvious piece of context—the name of the subreddit itself—leads to a geographically nonsensical and entirely incorrect data point.

#### **Data Redundancy and Duplication**

The dataset is plagued by redundant entries. Multiple rows for "Sighting near Northern Illinois" and "Sighting near Wisconsin" are exact duplicates across all fields, including the timestamp and source URL.1 This indicates a complete absence of a deduplication mechanism in the pipeline. Each time the source is processed, a new, identical entry is generated. Furthermore, the presence of both "Sighting near Chicago" and "Sighting near chicago" derived from the same source demonstrates a lack of basic text normalization (e.g., converting text to a consistent case) before processing or generating titles.1 This inflates the data volume and creates a misleading representation of event frequency on the map.

#### **Incorrect and Uninformative Temporal Extraction**

The Timestamp column across all entries appears to be the execution time of the data pipeline itself (e.g., 2025-10-07T07:35:53.907738Z), rather than the time of the actual event being described.1 The source text snippets contain explicit temporal information, such as "9/25/25 12:50pm," which is completely ignored by the current extraction process. This failure renders the temporal dimension of the map useless for any meaningful analysis, such as tracking event timelines, identifying patterns of activity, or understanding the temporal relationship between different sightings.

#### **Low Spatial Precision and Ambiguity**

Several entries are geocoded to vague, large-scale locations like "Northern Illinois" or "Wisconsin." These are resolved to a single, arbitrary coordinate pair, likely the geographic centroid of the region.1 For a map-based application intended to show specific sightings, a single point representing an entire state or a large portion of it provides no actionable or informative intelligence. The pipeline lacks a mechanism to either seek more precise location information from the text, handle such low-precision entities appropriately (e.g., by representing them as a bounding box), or flag them as ambiguous.

#### **Superficial and Incomplete Information Extraction**

The Title and Description fields are populated with generic, formulaic text (e.g., "Sighting near Chicago"). This approach fails to capture the rich, descriptive context available within the source snippets. Details such as "military-style raid on South Shore apartments," "Border Patrol on River headed north," or "checkpoint stop" are lost.1 This severely limits the utility of the map, reducing it to a collection of simple pins rather than a dashboard of informative event markers that answer not just *where* but also *what* happened.

#### **Empty or Unused Fields**

The VideoURL column is consistently empty across all records.1 Reddit posts, especially those documenting real-world events, frequently contain links to videos hosted on platforms like YouTube, Streamable, or Reddit's native video player.2 The consistently empty field suggests the pipeline either does not attempt to find video links or is incapable of correctly parsing and extracting them from the post body or associated comments.

### **1.2 Inferred Root Causes in the Existing Codebase/Pipeline**

The observed flaws are not random but point directly to specific architectural and logical failures in the data pipeline. The system appears to operate as a linear, stateless script rather than an intelligent, context-aware data processing workflow.

* **Ingestion:** The data is likely gathered via a simple keyword-based scraper targeting the r/chicago subreddit. Crucial metadata, such as the subreddit name, post flair, and full comment trees, is not being captured or passed to subsequent stages.  
* **Information Extraction:** A rudimentary process, likely based on regular expressions or simple keyword matching, is used to find the first string that looks like a location. There is no evidence of a sophisticated Named Entity Recognition (NER) model capable of understanding linguistic context. Temporal and event extraction capabilities are entirely absent.  
* **Geocoding:** The single, ambiguous location string is passed directly to a geocoding API without any contextual parameters (e.g., city, state). This is the direct cause of the geocoding failure that placed a Chicago-based event in the UK.  
* **Data Processing:** The pipeline evidently lacks any form of data deduplication, whether lexical (for exact duplicates) or semantic (for conceptually similar reports). Basic text normalization is also missing.  
* **Storage:** Data is written directly to a flat CSV file. This storage format lacks a proper schema, validation, and indexing capabilities, making it unsuitable for efficient querying, especially for geospatial operations required by a map application.

The interconnectedness of these failures reveals a deeper architectural issue. The geocoding error in the UK is not merely a bad lookup; it proves that the geocoding stage has no memory or context of the data's origin (the r/chicago subreddit). Similarly, the duplicate entries show that the pipeline has no concept of a unique "event" entity; it only processes individual "mentions." The incorrect timestamps confirm that the pipeline is only aware of its own execution time, not the time of the events it is supposed to be capturing. These failures across location, identity, and time all stem from an architecture that fails to maintain and utilize context from one stage to the next. The core problem is the absence of a holistic data model for a "sighting event."

Consequently, the data produced is not just of low quality; it is actively detrimental to the project's goals. A user of the map would be presented with misinformation—an event in the UK related to Chicago law enforcement, an exaggerated number of incidents in a given area due to duplication, and a completely false timeline of when events occurred. In its current state, the system generates a distorted and unreliable picture of reality, making it worse than having no data at all. The task of improving the pipeline is therefore not simply about acquiring "better data" but about transforming the system from a generator of misinformation into a source of genuine intelligence.

**Table 1: Diagnostic Analysis of map\_data.csv Output**

| Flaw ID | Flaw Category | Example from map\_data.csv | Description of Failure | Inferred Pipeline Deficiency |
| :---- | :---- | :---- | :---- | :---- |
| GEO-01 | Geocoding | "Sighting near Armitage" geocoded to 52.7417895, \-1.8775334 (UK). | The location is contextually incorrect. The source is r/chicago, and the text refers to an intersection in Chicago, IL. | Context-free geocoding API call; failure to use subreddit as a geographic prior. |
| DUP-01 | Deduplication | Multiple identical rows for "Sighting near Northern Illinois" from the same source URL. | The pipeline generates duplicate records for the same source information, artificially inflating event counts. | Absence of a deduplication stage (lexical or semantic). |
| NORM-01 | Normalization | "Sighting near Chicago" and "Sighting near chicago" from the same source. | Lack of case normalization leads to near-duplicate entries and inconsistent data representation. | No text normalization (e.g., lowercasing) in the processing stage. |
| TEMP-01 | Temporal Extraction | Timestamp is 2025-10-07T... for all entries, reflecting scrape time. | The pipeline fails to extract the actual event date/time (e.g., "9/25/25 12:50pm") from the text. | No temporal information extraction module; defaults to system time. |
| PREC-01 | Spatial Precision | "Sighting near Illinois" geocoded to a single point 40.6331249, \-89.3985283. | Vague regional mentions are represented by an imprecise and uninformative single point. | Inability to handle low-precision locations or extract bounding boxes. |
| INFO-01 | Information Extraction | Description contains generic text, ignoring details like "military-style raid on South Shore." | Rich contextual details about the nature of the event are not extracted, reducing data value. | Rudimentary entity extraction (likely regex); no event or argument extraction. |
| META-01 | Metadata Extraction | VideoURL column is consistently empty. | The pipeline fails to identify and extract links to relevant media within the source posts. | Incomplete data scraping; no logic to parse for media URLs. |

## **Section 2: Enhancing Data Provenance and Ingestion Strategy**

The quality of any data intelligence system is fundamentally constrained by the quality of its source data. The current pipeline's reliance on a single, unvetted social media source is a critical vulnerability. A robust architecture requires a deliberate strategy for data ingestion that acknowledges the inherent biases and challenges of the chosen sources, implements validation at the point of collection, and diversifies its inputs to build a more resilient and comprehensive view of events.

### **2.1 The "Double-Edged Sword": Analyzing Reddit as a Primary Data Source**

Reddit is a valuable source for real-time, user-generated information, but it is also a complex and challenging environment for data extraction. A successful pipeline must be designed with a clear understanding of its characteristics.

#### **Demographic and Behavioral Biases**

The Reddit user base is not a representative sample of the general population. Data indicates a significant skew towards younger (44% aged 18-29), male (59.8%) users, with a large concentration in the United States (49.59% of daily active users).2 This introduces a demographic bias; the events reported may disproportionately reflect the observations and concerns of this specific group. Furthermore, user motivation is a key factor. The primary reasons for using Reddit are entertainment (72%) and news consumption (43%), not systematic event reporting.3 This creates a participation bias, where events that are more sensational, visually interesting, or aligned with the community's interests are more likely to be posted.

#### **Linguistic Challenges**

Social media text is notoriously "noisy" and deviates significantly from the standardized language found in formal documents or news articles, upon which many standard NLP models are trained.4 Reddit content is characterized by non-standard language, including slang, community-specific jargon, acronyms (e.g., "TIL" for "Today I Learned"), intentional misspellings for humorous or stylistic effect, and the heavy use of emojis.5 This linguistic variance poses a significant challenge for NLP tasks like Named Entity Recognition, as models may fail to identify entities or may misinterpret slang terms.9

#### **Misinformation and Veracity**

Social media platforms are well-documented vectors for the spread of misinformation, rumors, and conspiracy theories.10 The current pipeline's implicit assumption that every mention of "ICE" or "CBP" corresponds to a factual, real-world sighting is a critical vulnerability. An effective system must incorporate mechanisms to assess the credibility of a source or claim, rather than blindly trusting all user-generated content. Without such vetting, the pipeline risks amplifying and legitimizing false information.

#### **Structural Advantages**

Despite these challenges, Reddit's structure offers a powerful, and currently unexploited, advantage: topic-specific communities known as subreddits.2 The name of a subreddit (e.g., r/chicago) provides strong, explicit context for both geography and topic. This single piece of metadata is a crucial signal for disambiguating locations, understanding the theme of a discussion, and even assessing the potential intent of a post. The failure of the current pipeline to capture and utilize this context is the root cause of its most severe errors.

### **2.2 A More Robust Ingestion Pipeline**

To mitigate these risks and leverage the platform's advantages, the ingestion stage of the pipeline must be re-architected to be more intelligent and comprehensive.

#### **Contextual Metadata Collection**

The ingestion script must be upgraded to collect a richer set of metadata alongside the post's text. This includes:

* **Subreddit Name:** The primary source of geographic and thematic context.  
* **Post Flair:** Many subreddits use flairs to categorize posts (e.g., "News," "Discussion," "Unverified"), which can be used as a feature for classification.  
* **User Metrics:** The author's account age, total karma, and subreddit-specific karma can serve as proxies for user credibility and community standing.  
* **Post Metrics:** The post's upvote score, upvote ratio, and number of comments indicate community engagement and potential significance.  
* **Full Comment Tree:** Comments often contain corrections, clarifications, additional sightings, or confirmations that are as valuable as the original post.

#### **Initial Noise Filtering and Source Vetting**

A preliminary filtering layer should be implemented at the point of ingestion to reduce the volume of low-quality data. Simple heuristics can be highly effective. For instance, posts from brand-new accounts, accounts with negative karma, or posts that are heavily downvoted can be flagged for review or discarded automatically. This acts as a first line of defense against spam, trolling, and low-effort posts, ensuring that more computationally expensive NLP processes are focused on higher-quality data.10

#### **Expanding the Scope**

The pipeline should move beyond a single subreddit like r/chicago. A discovery process should be undertaken to identify other relevant communities. This includes other regional subreddits (e.g., r/illinois, r/wisconsin, r/milwaukee), as well as topic-based subreddits focused on immigration rights, activism, and legal observation. This expansion will significantly increase the potential quantity and geographic diversity of the data collected.

### **2.3 Diversifying Data Sources with Public Event APIs**

Relying solely on Reddit creates a single point of failure and builds a system that is inherently subject to the platform's biases. To improve reliability, coverage, and the ability to validate information, the pipeline must be augmented with data from more structured, authoritative sources.

#### **The GDELT Project**

The Global Database of Events, Language, and Tone (GDELT) is a massive, open-source project that monitors news media from across the globe in over 100 languages and codifies events into a structured format.12 It extracts actors (e.g., government agencies), actions (e.g., protests, arrests), and locations for each event. By querying the GDELT API for events involving "Immigration and Customs Enforcement" or "Customs and Border Protection" within the target geographic areas, the pipeline can ingest a stream of more formal, structured event data. This data can be used to corroborate sightings reported on Reddit or to identify events missed by social media.

#### **News APIs (e.g., Event Registry)**

Commercial and open-source news APIs, such as NewsAPI.ai (Event Registry), provide real-time access to structured news data from thousands of sources.13 These platforms often employ their own advanced NLP models to pre-identify events, extract named entities, and categorize articles. Integrating such an API provides a high-quality, professionally curated stream of information that can serve as a baseline of verified events.14

#### **Alternative Data Sources**

Beyond news, other forms of "alternative data" can provide valuable context. Public APIs for government records, transportation data, or geospatial information can offer supplementary layers of intelligence.15 While more complex to integrate, these sources can help build a richer and more nuanced understanding of the operational environment.

The strategic choice of data sources is not a passive first step but an active design decision that defines the boundaries and inherent biases of the entire system. A pipeline built on a single source is fragile and skewed. A multi-source pipeline, in contrast, is more resilient and, crucially, can use one source to validate another. The data types are complementary: Reddit provides real-time, hyperlocal, and often firsthand accounts that are noisy and unverified, while news APIs offer professionally generated, structured, and generally verified information that may lack the immediacy or granular detail of social media reports. The goal is not merely to aggregate more data, but to build a composite view of reality by fusing data of different provenances and types. The pipeline should be architected to correlate events across these disparate sources, using a news report to confirm a Reddit sighting, or a cluster of Reddit posts to flag an emerging event that has not yet hit the news cycle.

## **Section 3: A Multi-Stage NLP Framework for High-Fidelity Information Extraction**

To transform the raw, noisy text from ingestion into structured, actionable data, the pipeline requires a sophisticated processing engine. This involves moving beyond the current system's rudimentary keyword matching to a multi-stage Natural Language Processing (NLP) framework. This framework will be responsible for three core tasks: accurately identifying locations, precisely extracting event times, and richly describing the nature of the event itself. These tasks are interconnected, and the framework must be designed to allow the output of one to inform the others, creating a holistic understanding of the text.

### **3.1 Context-Aware Location Entity Recognition (NER)**

The primary failure of the current system is its inability to correctly identify and disambiguate locations. A robust solution requires a modern NER approach that can handle the unique challenges of social media text.

#### **The Problem with Generic NER**

Standard, off-the-shelf NER models are often trained on clean, well-structured text like news articles or Wikipedia entries.17 When applied to the noisy, informal, and context-dependent language of social media, their performance can degrade significantly.4 An entity like "Central Park" can refer to distinct locations in New York City, Chicago, or dozens of other municipalities. A generic model, lacking specific contextual cues, cannot reliably disambiguate such references.19 The current pipeline's failure to distinguish "Armitage" in Chicago from "Armitage" in the UK is a clear example of this problem.1

#### **Proposed Solution: Fine-Tuning a Transformer-Based Model**

The state-of-the-art approach to this problem is to adapt a powerful, pre-trained transformer model to the specific domain of the project's data.

1. **Model Selection:** The process should begin with a strong baseline model. A model like dslim/bert-base-NER, available through the Hugging Face ecosystem, is an excellent starting point. It is a BERT (Bidirectional Encoder Representations from Transformers) model that has already been fine-tuned on the CoNLL-2003 NER dataset and is capable of identifying Location (LOC), Organization (ORG), and Person (PER) entities out of the box.17 The key advantage of BERT is its bidirectional architecture, which allows it to consider the full context of a word—both preceding and following—to understand its meaning, a significant improvement over older, unidirectional models.22  
2. **Domain Adaptation through Custom Annotation:** To overcome the limitations of the generic training data, a custom-annotated dataset must be created. This involves taking a representative sample of the project's own Reddit data and manually labeling the location entities. This process is critical for teaching the model the specific linguistic patterns and types of locations relevant to this use case, such as street intersections ("Central Park and Armitage"), informal neighborhood names, and local landmarks that would not be present in a generic dataset.18  
3. **Fine-Tuning:** The pre-trained BERT model is then further trained (or "fine-tuned") on this new, custom-annotated dataset. This process adjusts the model's internal weights to specialize its performance for the unique vocabulary, syntax, and geographic context of the target Reddit communities.25  
4. **Leveraging Subreddit Context:** To maximize accuracy, the NER process must be made explicitly context-aware. Before a post's text is fed to the model for inference, the name of the subreddit (e.g., "chicago") should be prepended to the text. This provides a powerful, unambiguous hint to the model, helping it disambiguate locations and ground its predictions in the correct geographic area.

#### **Alternative: spaCy for a Lighter-Weight Implementation**

For applications where computational resources are more constrained or where faster prototyping is a priority, the spaCy library offers an excellent alternative. SpaCy's NER models provide a strong balance of accuracy, speed, and ease of use.28 While a pre-trained spaCy model may be less accurate on highly noisy text compared to a fine-tuned BERT model, its performance can be significantly improved by training it on the same custom-annotated dataset created for the BERT fine-tuning process.31

**Table 2: Comparative Analysis of NER Models for Location Extraction**

| Model/Approach | Pros | Cons | Performance on Noisy Text | Implementation Complexity | Recommendation for this Project |
| :---- | :---- | :---- | :---- | :---- | :---- |
| Regex/Keywords | Simple to implement; fast execution. | Brittle; cannot handle variations; high false positives/negatives. | Low | Low | Not recommended. This is likely the current failed approach. |
| Pre-trained spaCy | Easy to use; good balance of speed and accuracy; good baseline. | May struggle with domain-specific jargon and ambiguity. | Medium | Low | Recommended for initial prototyping (Phase 1). |
| Custom-trained spaCy | Improved accuracy on domain-specific text; still relatively fast. | Requires manual data annotation; less powerful than transformers. | High | Medium | A viable production option if BERT is too resource-intensive. |
| Pre-trained BERT | High accuracy on general text; good contextual understanding. | Slower inference; may still fail on highly specific slang/jargon. | High | Medium | Good baseline, but fine-tuning is necessary for optimal performance. |
| Fine-tuned BERT | State-of-the-art accuracy; adapts to noisy, domain-specific language. | Computationally expensive to train/run; requires data annotation. | Very High | High | **Recommended for the production system (Phase 2/3).** |

### **3.2 Precise Temporal Information Extraction**

A critical flaw in the current data is the complete absence of event-specific timestamps.1 Extracting and normalizing temporal expressions from unstructured text is a complex but solvable NLP task.35

#### **The Challenge of Temporal Expressions**

Event times mentioned in social media posts can take many forms:

* **Absolute:** "September 25, 2025 at 12:50pm"  
* **Relative:** "two days ago," "yesterday morning," "last week"  
* **Imprecise:** "in the evening," "around noon"

The pipeline must be able to parse all these forms and convert them into a standardized, machine-readable format (e.g., an ISO 8601 timestamp).

#### **Proposed Solution: A Specialized Date Parsing Library**

Instead of building a complex temporal parser from scratch, the pipeline should leverage a specialized, robust library designed for this purpose.

1. **Implement dateparser:** The dateparser library for Python is an excellent choice. It is designed to parse human-readable dates from strings and can handle a vast array of formats, relative time expressions, and multiple languages.38  
2. **Anchor Relative Dates:** The key to correctly interpreting relative expressions like "yesterday" or "2 hours ago" is to provide an anchor time. The creation date of the Reddit post, which should be collected during the ingestion stage, must be passed to dateparser's parse function via the relative\_base setting. This allows the library to correctly calculate the absolute timestamp (e.g., "yesterday" relative to a post made on Oct 7th becomes Oct 6th).41  
3. **Handle Ambiguity:** The library's settings should be configured to handle common ambiguities. For example, setting PREFER\_DATES\_FROM: 'past' will ensure that a mention of "Friday" in a post on a Sunday is interpreted as the preceding Friday, not the upcoming one, which is more likely in the context of reporting a past event.39

For more advanced use cases involving complex temporal relationships between multiple events within a single document, a library like pyTLEX, which constructs a full timeline graph from TimeML-annotated text, could be considered. However, for the immediate goal of extracting a single event time per post, dateparser is the most direct and effective solution.42

### **3.3 Event and Argument Extraction for Richer Data**

To move beyond simple pins on a map, the pipeline must extract *what* happened. This task, known as event extraction, involves identifying an event trigger (the key action word or phrase) and its associated arguments (the participants, objects, etc.).43

#### **Proposed Solution: From Dependency Parsing to Dedicated Models**

1. **Simple Approach (Dependency Parsing):** A highly effective initial approach can be built using spaCy's dependency parser. After identifying the key entities ("ICE," "Border Patrol"), the parser can be used to analyze the grammatical structure of the sentence. By finding the verb associated with the agency entity, the system can extract its subject, object, and related prepositional phrases to construct a simple event description.32 For example, in "Border Patrol on River headed north," the parser can identify "Border Patrol" as the subject of the verb "headed," and extract "on River" and "north" as descriptive modifiers.  
2. **Advanced Approach (Event Extraction Models):** For greater accuracy and the ability to classify events into predefined types (e.g., RAID, CHECKPOINT, ARREST), a dedicated event extraction model can be trained. This would follow a similar process to the NER model: annotate a dataset with event triggers and arguments, and then fine-tune a transformer-based model to recognize these structured patterns automatically.

The three NLP tasks of location, time, and event extraction should not be viewed as independent, sequential steps. They are deeply interconnected, and an intelligent pipeline will use the outputs of each to refine the others. For example, the identification of an event trigger like "raid" increases the probability that a nearby location entity like "South Shore apartments" is the specific site of the event, not just a passing mention. Similarly, extracting a precise time helps disambiguate which of several potential events in a long post is the primary one being reported. The final output of the NLP module should not be a loose collection of strings but a structured JSON object that represents a single, coherent event, capturing the who, what, when, and where in a unified data model. This structured object then becomes the clean, context-rich input for the downstream geocoding and deduplication stages.

## **Section 4: Achieving High-Precision Geocoding and Spatial Context**

Once the NLP framework has extracted high-quality, context-aware location entities, the next critical stage is to convert these textual descriptions into precise and, most importantly, correct latitude and longitude coordinates. This process, known as geocoding, is far more than a simple API call; it is a disambiguation and resolution step where precision and accuracy are paramount. The failures of the current system highlight the pitfalls of a naive approach, and a robust pipeline must implement a series of best practices to ensure geographic fidelity.

### **4.1 Implementing Best Practices for Geocoding APIs**

The selection and proper use of a geocoding service are foundational to achieving accurate results. The quality of the geocoder's output is almost entirely dependent on the quality and context of its input.

#### **Choosing the Right API and Constructing Unambiguous Queries**

For processing ambiguous or unstructured address strings, the Google Maps Geocoding API is a powerful and appropriate choice. While the Places API is often recommended for latency-sensitive applications with real-time user input, the Geocoding API is well-suited for the automated, batch-processing nature of this pipeline.45

The single most impactful improvement to the geocoding process is the construction of unambiguous, context-rich queries. The failure to do this is the direct cause of the "Armitage, UK" error in the current dataset.1 Instead of sending an isolated, ambiguous toponym like "Armitage" to the API, the query must be programmatically constructed using the information gathered in the preceding stages. The query string should be a concatenation of the extracted location entity and the geographic context derived from the subreddit.

* **Poor Query (Current Method):** address=Armitage  
* **Improved Query (Proposed Method):** address=Central Park and Armitage, Chicago, IL, USA

By providing the city, state, and country, the query space is dramatically constrained, virtually eliminating the possibility of such gross errors and guiding the geocoding service to the correct result.46

#### **Handling API Usage, Errors, and Response Validation**

Professional-grade interaction with any external API requires robust handling of technical protocols and potential failures.

1. **URL Encoding:** All query strings sent to the API must be properly URL-encoded. This standard practice ensures that spaces, special characters, and non-ASCII characters are transmitted correctly and conform to URI specifications, preventing request failures.47  
2. **Error Handling and Retries:** API calls can fail for transient reasons, such as network timeouts or temporary service unavailability. A resilient pipeline should not terminate on a single failed request. Instead, it must implement a retry mechanism with exponential backoff. This strategy involves re-issuing a failed request after a short delay, and multiplicatively increasing the delay with each subsequent failure. This prevents overwhelming the API service while gracefully handling temporary issues.45  
3. **Parsing and Validating Responses:** The pipeline must not blindly accept the coordinates from an API response. It should first parse the JSON or XML response and inspect the status field to confirm the request was successful. Furthermore, it should examine the location\_type attribute of the result. A value of ROOFTOP or RANGE\_INTERPOLATED indicates a high-precision result (e.g., a specific address). In contrast, a value of APPROXIMATE signals a lower-precision result, which is common for vague queries like "Northern Illinois."

For these low-precision results, the current pipeline's practice of plotting a single, arbitrary point is misleading. A much more accurate and honest representation is to extract the viewport or bounds object from the API response, which defines a bounding box for the geographic area. This polygon data should be stored alongside the result and used by the frontend application to render a shaded area on the map, visually communicating the inherent uncertainty and imprecision of the location information. This approach transforms the map from a collection of potentially inaccurate points into a more truthful visualization of the available data.

### **4.2 Advanced Disambiguation with Knowledge Graphs**

While contextual query construction solves many ambiguity problems, some complex cases remain. For instance, a post in the r/illinois subreddit mentioning an event in "Springfield" is still ambiguous, as there are multiple cities named Springfield in the United States. While the subreddit provides a strong hint, more advanced techniques can provide near-certain resolution. A forward-looking approach is to use a knowledge graph.

A knowledge graph is a data structure that represents a network of real-world entities (such as people, organizations, and places) and the relationships between them.48 By building a simple knowledge graph from the data being processed, the pipeline can leverage co-occurrence patterns to perform powerful, context-aware disambiguation.

#### **Example Implementation Workflow**

1. **Entity Graph Construction:** As the pipeline processes Reddit posts, it uses the NLP module to extract all named entities—not just locations, but also people, organizations, and events.  
2. **Relationship Linking:** These entities are stored as nodes in a graph. An edge is created between two nodes if the corresponding entities co-occur within the same post or comment. The weight of the edge can be increased with each co-occurrence.  
3. **Contextual Inference:** When the pipeline encounters an ambiguous location like "Springfield," it can query the knowledge graph to find its neighboring nodes. If the "Springfield" node is strongly connected to other entities known to be in Illinois, such as the "Abraham Lincoln Presidential Library" or the "Illinois State Capitol," the system can infer with very high confidence that the reference is to Springfield, Illinois.

This method provides a powerful, scalable framework for resolving complex geographic ambiguities by mimicking a form of human reasoning. It turns the entire corpus of processed data into a contextual background that can be used to disambiguate new information, creating a self-improving system where more data leads to better geocoding accuracy.

## **Section 5: Guaranteeing Data Integrity via a Multi-Layered Deduplication Strategy**

The analysis of map\_data.csv revealed a critical failure in data integrity: the presence of numerous exact and near-duplicate records.1 This issue inflates event counts, skews any potential analysis, and clutters the map interface with redundant information. To ensure data integrity, a robust pipeline must incorporate a sophisticated, multi-layered deduplication strategy. This strategy should address not only simple lexical duplicates (identical or very similar text) but also more complex semantic duplicates (different texts describing the same real-world event). Implementing such a system will fundamentally transform the dataset from a simple log of mentions into a curated database of unique events.

### **5.1 Layer 1: High-Speed Lexical Deduplication**

The first layer of defense is a high-speed process designed to eliminate exact and near-duplicate text entries. This addresses the most straightforward cases of redundancy, such as multiple identical rows generated from processing the same source or minor variations due to casing and punctuation.

#### **Technique: MinHash LSH (Locality-Sensitive Hashing)**

MinHash LSH is a powerful and highly efficient probabilistic algorithm ideal for identifying near-duplicate documents in large datasets.51 It avoids the computationally prohibitive task of comparing every document to every other document by using a clever hashing technique to group similar documents together.

1. **Text Normalization:** The process begins with rigorous text normalization. All text content from the posts is converted to a consistent format: converted to lowercase, all punctuation and special characters removed, and whitespace standardized. This ensures that superficial differences like "Chicago." and "chicago" do not prevent the documents from being recognized as duplicates.52  
2. **Shingling:** The normalized text is then deconstructed into a set of overlapping, fixed-size character sequences known as shingles (or n-grams). For example, the string "the cat" with a shingle size of 3 would produce the set {'the', 'he ', 'e c', ' ca', 'cat'}. This method captures the structure of the text while being resilient to minor changes.54  
3. **MinHashing:** The set of shingles for each document is then converted into a compact, fixed-size numerical signature using the MinHash algorithm. This signature cleverly approximates the Jaccard similarity between the original shingle sets, allowing for fast comparison.52  
4. **Locality-Sensitive Hashing (LSH):** The final step involves hashing portions of the MinHash signatures into a set of buckets. The LSH algorithm is designed such that documents with similar MinHash signatures have a high probability of being hashed into the same bucket. This means that instead of comparing every document pair, the system only needs to compare the documents that fall into the same buckets, drastically reducing the computational complexity.51

#### **Implementation**

This high-performance lexical deduplication layer can be effectively implemented using the datasketch library in Python. This library provides optimized and easy-to-use implementations of the MinHash and MinHashLSH algorithms, making it an ideal choice for this initial, high-speed filtering stage.54

### **5.2 Layer 2: Semantic Deduplication for Conceptual Redundancy**

While lexical deduplication cleans up identical mentions, it cannot identify a more subtle and important form of redundancy: multiple, distinct posts that describe the very same real-world event using different language. For example, "ICE raid on South Shore" and "CBP conducted an operation in the South Shore neighborhood" refer to the same underlying event but are textually different.58 Failing to merge these conceptual duplicates leads to a significant over-counting of actual events.

#### **Technique: Sentence Embeddings with Vector Similarity Search**

This advanced layer addresses deduplication at the level of meaning, or semantics.

1. **Sentence Embeddings with Sentence-BERT:** The core of this technique is to convert the textual content of each sighting into a high-dimensional vector, known as an embedding. A Sentence-BERT (SBERT) model is perfectly suited for this task. SBERT is a modification of the BERT model designed to produce semantically meaningful embeddings for entire sentences or paragraphs. The resulting vectors are positioned in a high-dimensional space such that texts with similar meanings are located close to one another.60 The sentence-transformers Python library provides a wide range of pre-trained SBERT models and a simple API for generating these embeddings.61  
2. **Efficient Similarity Search with FAISS:** As the database of sightings grows into the thousands or millions, searching for the nearest vector neighbors for each new entry becomes a significant computational bottleneck. FAISS (Facebook AI Similarity Search) is a specialized library designed for extremely fast similarity searches on massive collections of vectors.66 It uses techniques like quantization and indexing to approximate the nearest neighbor search, enabling lookups in milliseconds even across billions of vectors.69

#### **Implementation Workflow**

The semantic deduplication process follows a clear workflow, often referred to as a SemDeDup-style approach 75:

1. As a new, lexically unique sighting enters the pipeline, its textual description is encoded into a Sentence-BERT embedding.  
2. This new embedding is then used as a query against a FAISS index that contains the embeddings of all previously processed, unique events.69  
3. FAISS rapidly returns the most similar embeddings already in the index, along with their similarity scores (e.g., cosine similarity).  
4. If any of the returned neighbors have a similarity score above a predefined threshold (e.g., \> 0.9), the new sighting is classified as a semantic duplicate of an existing event.  
5. Instead of creating a new, redundant event in the database, the system links this new mention to the existing event record. This can be used to update a "mention count" or "confidence score" for the original event, enriching it with new information without creating a duplicate map point.

This two-layer deduplication strategy fundamentally redefines the nature of the dataset. It transforms the data from a simple "log of text mentions" into a structured "database of unique real-world events." This is a critical conceptual shift. The primary identifier in the database effectively changes from a post\_id to a unique event\_id. Each event can then have a collection of mentions (the individual Reddit posts) associated with it. This normalized data model is vastly more powerful for analysis, enabling the system to answer crucial questions like, "How many distinct events occurred last week?" versus "How many posts were made last week?" and "Which event generated the most social media discussion?"

Furthermore, the combination of semantic deduplication with the temporal and spatial data extracted in earlier stages creates a highly robust event clustering mechanism. True duplicates are not just semantically similar; they are also proximate in time and space. Two posts describing "an ICE van" are semantically similar, but if one reports the van in the "Pilsen" neighborhood at 2:00 PM and the other reports it in "Rogers Park" at 2:05 PM, they are clearly distinct events. Conversely, a post about "a CBP vehicle in Pilsen around 2:15 PM" is highly likely to be the same event as the first post. Therefore, the logic for identifying a duplicate should not rely on text alone. A more sophisticated similarity score should be a weighted function of semantic similarity, temporal proximity, and spatial proximity:

where  is the cosine similarity of the embeddings,  are the event timestamps,  are the geocoded points, and  and  are functions that decay with increasing time and distance. This multi-modal approach to similarity ensures a much more accurate and contextually aware clustering of mentions into unique real-world events.

**Table 3: Deduplication Strategy and Technology Comparison**

| Layer | Objective | Primary Technology | Python Library | Pros | Cons |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **Lexical** | Remove exact and near-duplicate posts based on textual identity. | MinHash LSH | datasketch | Extremely fast; scales to massive datasets; low computational overhead. | Misses semantic similarity; cannot identify paraphrased or conceptually similar content. |
| **Semantic** | Group different posts that describe the same real-world event. | Sentence-BERT \+ FAISS | sentence-transformers, faiss-cpu | Catches conceptual duplicates; understands meaning and context. | Computationally intensive (requires embeddings and vector search); more complex to implement. |

## **Section 6: A Resilient and Scalable Data Pipeline Architecture**

Synthesizing the recommendations from the preceding sections, this section presents a blueprint for a modern, resilient, and scalable data pipeline architecture. This design moves away from a simple, fragile script and towards a professional, orchestrated workflow that incorporates best practices in data engineering. The proposed architecture is modular, automated, and observable, ensuring the production of high-fidelity geospatial data in a maintainable and scalable manner.

### **6.1 Blueprint for a Modernized ETL Pipeline**

The proposed pipeline follows a modern Extract, Transform, Load (ETL) pattern, with distinct, modular stages designed for clarity and independent operation.

1. **Ingestion:** This stage is responsible for data acquisition. Scheduled tasks connect to data sources—primarily the Reddit API, but also augmented sources like the GDELT API or other news APIs. The key principle here is to extract the raw data in its entirety, including all associated metadata (subreddit, user info, timestamps, comment trees), and load it into a data lake, such as a bucket in Google Cloud Storage (GCS) or Amazon S3. This preserves the original data for reprocessing or auditing and decouples the acquisition process from the transformation logic.78  
2. **Staging & Lexical Deduplication:** From the data lake, new raw data is loaded into a staging area (e.g., a temporary database table or in-memory dataframe). Here, the high-speed lexical deduplication process using MinHash LSH is executed. This acts as a fast, initial filter to remove exact and near-duplicate mentions before they enter the more computationally expensive stages of the pipeline.  
3. **Transformation (NLP Enrichment):** This is the core processing stage. The lexically unique records are processed by the multi-stage NLP framework. The fine-tuned BERT model performs Named Entity Recognition, the dateparser library extracts and normalizes temporal information, and dependency parsing or an event model extracts the event's nature. The output of this stage is a structured representation of each mention, with discrete fields for locations, times, and event descriptions.  
4. **Resolution (Geocoding & Semantic Deduplication):** The enriched records now undergo resolution. The structured location data is passed to the geocoding API to obtain precise coordinates and bounding boxes. Following this, the semantic deduplication process is executed. The text is converted to a Sentence-BERT embedding and queried against the FAISS index of existing unique events. If a match is found, the mention is linked to the existing event. If not, a new unique event record is created.  
5. **Loading (Final Storage):** The final, structured, and deduplicated event data is loaded into a production database that is optimized for the application's query patterns. This final data store serves as the single source of truth for the map application.

### **6.2 Orchestration, Monitoring, and Automation with Apache Airflow**

A simple cron job is insufficient for managing a multi-stage pipeline of this complexity. A dedicated workflow orchestration tool is required to ensure reliability, observability, and automation. Apache Airflow is the industry standard for this purpose.

#### **Why Airflow?**

Airflow allows developers to define complex data pipelines as Python code, in the form of Directed Acyclic Graphs (DAGs). It manages the scheduling of these pipelines, handles dependencies between tasks, automatically retries failed tasks, and provides a rich user interface for monitoring and managing executions.79 Adopting Airflow represents a paradigm shift towards treating data engineering with the same rigor as software engineering, introducing principles of version control, modularity, and automated testing to the data workflow.81

#### **Structuring the Pipeline as a DAG**

The proposed pipeline will be implemented as an Airflow DAG, where each stage is a distinct task or group of tasks. This modularity is crucial for maintainability and debugging.

**Example DAG Structure (event\_extraction\_dag.py):**

* ingest\_sources\_task: A PythonOperator that executes the scripts to scrape Reddit and query other APIs, saving the raw data to the data lake.  
* lexical\_deduplication\_task: A PythonOperator that reads from the data lake, performs MinHash LSH deduplication, and writes the unique records to a new location.  
* nlp\_enrichment\_task: A PythonOperator that applies the NER, temporal, and event extraction models to the unique records. This task may require a Docker or Kubernetes operator if it has heavy dependencies or requires significant resources.  
* geocoding\_and\_semantic\_dedup\_task: A PythonOperator that handles geocoding and uses FAISS to resolve semantic duplicates, preparing the final event data.  
* load\_to\_production\_db\_task: A task using a specific provider hook (e.g., ElasticsearchPythonHook) to efficiently load the final structured data into the production database.

These tasks will be linked with dependencies (\>\>) to define the execution order, ensuring that, for example, NLP enrichment only begins after lexical deduplication is complete.82 Airflow's UI will provide detailed logs for each task run, alerts on failure, and a clear visual representation of the pipeline's status, offering a vast improvement in operational maturity.

### **6.3 Optimized Data Storage for Geospatial Querying**

The choice of the final data store is a critical architectural decision that directly enables or disables the desired application functionality. A flat CSV file is fundamentally unsuited for a production map application that requires efficient, interactive spatial querying.

#### **Proposed Solution: Elasticsearch**

Elasticsearch is a distributed search and analytics engine that is exceptionally well-suited for this use case due to its powerful geospatial capabilities.

1. **geo\_point Data Type:** The most critical step is to define the index mapping in Elasticsearch to specify that the location field is of type geo\_point. This simple declaration enables Elasticsearch to build specialized data structures (BKD-trees) to index the geospatial data for extremely fast querying. This is a common point of failure for developers new to Elasticsearch, as without the correct mapping, location data is treated as simple numbers, and geo queries will not work.86  
2. **Geospatial Queries:** Once the data is indexed with the correct mapping, the application's backend can leverage Elasticsearch's comprehensive Geo Query DSL. This includes queries like geo\_distance to find all events within a specified radius of a central point (e.g., "find all sightings within 5 km of my current location") and geo\_bounding\_box to find all events within the rectangular area of the user's current map view.87 These capabilities are essential for building a responsive and interactive map experience.  
3. **Implementation:** The load\_to\_production\_db\_task in the Airflow DAG will use the official elasticsearch-py client library to index the final, structured JSON documents representing the unique events. The operator will connect to the Elasticsearch cluster and bulk-insert the data, ensuring efficient loading.90

The architectural decision to use a specialized data store like Elasticsearch is not an afterthought; it is a core requirement driven by the application's primary function. The data pipeline and the end-user application must be designed in concert. By designing the pipeline to produce data in a format and location optimized for the application's query patterns, the system ensures that the "better data" it produces can actually be used effectively.

**Table 4: Proposed Pipeline Technology Stack**

| Pipeline Stage | Primary Technology/Tool | Key Python Library/SDK | Justification |
| :---- | :---- | :---- | :---- |
| **Orchestration** | Apache Airflow | apache-airflow | Industry standard for defining, scheduling, and monitoring complex data workflows as code. Ensures reliability and observability. |
| **Ingestion** | Reddit API / News APIs | praw, requests | Direct access to raw social media data and structured news data for source diversification and validation. |
| **NLP: NER** | BERT (fine-tuned) | transformers (Hugging Face) | State-of-the-art performance for NER on noisy, domain-specific text after fine-tuning. |
| **NLP: Temporal** | Dateparser | dateparser | Robustly parses a wide variety of absolute and relative human-readable date/time expressions. |
| **Deduplication (Lexical)** | MinHash LSH | datasketch | Highly efficient probabilistic method for finding near-duplicate text documents at scale. |
| **Deduplication (Semantic)** | Sentence-BERT \+ FAISS | sentence-transformers, faiss-cpu | State-of-the-art for identifying conceptually similar documents by comparing semantic embeddings. FAISS enables high-speed search. |
| **Geocoding** | Google Geocoding API | google-maps-services-python | Robust and accurate for converting ambiguous textual locations into geographic coordinates, especially when provided with context. |
| **Storage** | Elasticsearch | elasticsearch-py | Optimized for full-text search and high-performance geospatial queries, which are essential for the map application's backend. |

## **Section 7: Summary of Recommendations and Implementation Roadmap**

This report has conducted a comprehensive analysis of the existing data pipeline, identified critical flaws in its output, and proposed a re-architecture based on modern data engineering and NLP best practices. The goal of this new architecture is to transform the system from a simple script into a robust, intelligent pipeline capable of producing high-fidelity, accurate, and context-rich geospatial event data. This final section consolidates the key recommendations and presents a phased implementation roadmap to guide the transition from the current state to the target architecture.

### **7.1 Consolidated Recommendations**

The following is a summary of the core architectural and methodological changes recommended to achieve the project's goals:

* **Adopt a Multi-Source Ingestion Strategy:** Cease reliance on a single social media source. Augment Reddit data by integrating structured event data from news APIs like GDELT or Event Registry to improve data veracity and coverage.  
* **Implement Contextual Metadata Collection:** Enhance the ingestion process to capture crucial metadata from Reddit, including subreddit name, post flair, and user/post metrics, to be used for context in downstream processing.  
* **Deploy a Multi-Stage NLP Framework:** Replace rudimentary text matching with a sophisticated NLP module that includes:  
  * **Location Extraction:** A fine-tuned BERT model for high-accuracy Named Entity Recognition on noisy social media text.  
  * **Temporal Extraction:** The dateparser library to accurately parse absolute and relative time expressions, anchored to post creation times.  
  * **Event Extraction:** Dependency parsing or a dedicated model to extract the nature of the event beyond a simple "sighting."  
* **Revamp the Geocoding Process:** Construct unambiguous queries for the geocoding API by combining extracted location entities with geographic context from the source (e.g., subreddit name). Implement robust error handling and response validation, including the capture of bounding boxes for low-precision results.  
* **Integrate a Two-Layer Deduplication System:**  
  * **Lexical:** Use MinHash LSH to perform a fast initial pass to eliminate exact and near-duplicate text mentions.  
  * **Semantic:** Use Sentence-BERT embeddings and a FAISS vector index to identify and merge conceptually similar reports, creating a database of unique events rather than redundant mentions.  
* **Re-architect the Workflow with an Orchestrator:** Implement the entire pipeline as a Directed Acyclic Graph (DAG) using Apache Airflow. This will provide robust scheduling, dependency management, automated retries, monitoring, and logging.  
* **Migrate to a Geospatial-Optimized Data Store:** Transition the final data storage from a flat CSV file to Elasticsearch. Define a proper index mapping with a geo\_point field to enable efficient, high-performance geospatial queries required by the map application.

### **7.2 Phased Implementation Roadmap**

A complete overhaul can be a daunting task. The following phased roadmap breaks down the implementation into manageable stages, prioritizing the most critical fixes first while building towards the full target architecture.

#### **Phase 1: Immediate Triage (1-2 Weeks)**

**Goal:** To rapidly address the most severe data quality issues and eliminate fundamentally incorrect data with minimal architectural changes. This phase focuses on "stopping the bleeding."

**Actions:**

1. **Modify Ingestion Script:** Update the existing scraper to capture and store the subreddit name for each post.  
2. **Update Geocoding Call:** Immediately change the geocoding function to concatenate the extracted location with the city/state derived from the subreddit name before sending the query to the API. This will fix the UK geocoding error.  
3. **Implement Basic Deduplication:** Add a simple lexical deduplication step. Before writing a new record, check if a record with the same SourceURL and normalized text content already exists. A dictionary or a set can be used to track processed items within a single run.  
4. **Add Text Normalization:** Implement basic text normalization (e.g., converting relevant fields to lowercase) before processing to handle simple near-duplicates.

#### **Phase 2: Core NLP and Data Model Overhaul (1-3 Months)**

**Goal:** To build the foundation of the new intelligent pipeline by implementing the core NLP framework, establishing the "unique event" data model, and setting up the proper storage and orchestration infrastructure.

**Actions:**

1. **Set Up Infrastructure:** Deploy instances of Apache Airflow and Elasticsearch.  
2. **Data Annotation:** Begin the process of manually annotating a sample of Reddit data. Use an annotation tool (e.g., Doccano, Label Studio) to label location entities and temporal expressions.  
3. **Implement NLP Modules:**  
   * Develop and integrate the dateparser logic for temporal extraction.  
   * Train an initial version of a custom NER model (either fine-tuning BERT or training a spaCy model) using the annotated data.  
4. **Define Storage Schema:** Design and implement the Elasticsearch index mapping, ensuring the location field is correctly defined as geo\_point and other fields have appropriate data types.  
5. **Build Initial Airflow DAG:** Create a basic Airflow DAG that orchestrates the new components: ingestion, NLP enrichment, geocoding, and loading into Elasticsearch.

#### **Phase 3: Advanced Features and Scalability (3-6+ Months)**

**Goal:** To implement the advanced deduplication capabilities, incorporate multiple data sources, and mature the pipeline into a fully resilient, scalable, and production-grade system.

**Actions:**

1. **Implement Semantic Deduplication:** Integrate the Sentence-BERT embedding and FAISS indexing workflow into the Airflow DAG. Refine the data model to distinguish between "events" and "mentions."  
2. **Incorporate Alternative Data Sources:** Add new tasks to the Airflow DAG to ingest and process data from the GDELT Project and/or a commercial news API.  
3. **Develop Event Correlation Logic:** Design and implement logic to correlate events across different sources (e.g., matching a Reddit post to a news article based on location, time, and semantic similarity).  
4. **Enhance Orchestration:** Implement advanced Airflow features, including robust monitoring, alerting on failures, data quality checks, and dynamic task generation.  
5. **Optimize and Scale:** Profile the performance of each pipeline stage. Optimize computationally intensive tasks (e.g., by using GPU-enabled workers for model inference and FAISS) and ensure the architecture can scale to handle increasing data volumes.  
6. **Explore Future Enhancements:** Begin research and prototyping for more advanced features, such as the knowledge graph-based disambiguation system.

#### **Works cited**

1. map\_data.csv  
2. Reddit Statistics: Tactics to Grow Your Brand \[2025\] | Sprout Social, accessed October 7, 2025, [https://sproutsocial.com/insights/reddit-statistics/](https://sproutsocial.com/insights/reddit-statistics/)  
3. Reddit User and Growth Stats (Updated) \- Backlinko, accessed October 7, 2025, [https://backlinko.com/reddit-users](https://backlinko.com/reddit-users)  
4. Named Entity Recognition for Social Media Text \- DiVA portal, accessed October 7, 2025, [http://www.diva-portal.org/smash/get/diva2:1366031/FULLTEXT01.pdf](http://www.diva-portal.org/smash/get/diva2:1366031/FULLTEXT01.pdf)  
5. (PDF) Natural Language Processing for Social Media \- ResearchGate, accessed October 7, 2025, [https://www.researchgate.net/publication/345803153\_Natural\_Language\_Processing\_for\_Social\_Media](https://www.researchgate.net/publication/345803153_Natural_Language_Processing_for_Social_Media)  
6. A LINGUISTIC ANALYSIS OF THE EVOLUTION OF INTERNET SLANG \- Innovative Academy, accessed October 7, 2025, [https://in-academy.uz/index.php/cajmrms/article/download/41209/26220/43766](https://in-academy.uz/index.php/cajmrms/article/download/41209/26220/43766)  
7. Exploring register variation on Reddit: A multi-dimensional study of language use on a social media website \- ResearchGate, accessed October 7, 2025, [https://www.researchgate.net/publication/336040217\_Exploring\_register\_variation\_on\_Reddit\_A\_multi-dimensional\_study\_of\_language\_use\_on\_a\_social\_media\_website](https://www.researchgate.net/publication/336040217_Exploring_register_variation_on_Reddit_A_multi-dimensional_study_of_language_use_on_a_social_media_website)  
8. (PDF) On the use of Jargon and Word Embeddings to Explore Subculture within the Reddit's Manosphere \- ResearchGate, accessed October 7, 2025, [https://www.researchgate.net/publication/342402739\_On\_the\_use\_of\_Jargon\_and\_Word\_Embeddings\_to\_Explore\_Subculture\_within\_the\_Reddit's\_Manosphere](https://www.researchgate.net/publication/342402739_On_the_use_of_Jargon_and_Word_Embeddings_to_Explore_Subculture_within_the_Reddit's_Manosphere)  
9. (PDF) Comparative Analysis of Reddit Posts and ChatGPT-Generated Texts' Linguistic Features: A Short Report on Artificial Intelligence's Imitative Capabilities \- ResearchGate, accessed October 7, 2025, [https://www.researchgate.net/publication/385754265\_Comparative\_Analysis\_of\_Reddit\_Posts\_and\_ChatGPT-Generated\_Texts'\_Linguistic\_Features\_A\_Short\_Report\_on\_Artificial\_Intelligence's\_Imitative\_Capabilities](https://www.researchgate.net/publication/385754265_Comparative_Analysis_of_Reddit_Posts_and_ChatGPT-Generated_Texts'_Linguistic_Features_A_Short_Report_on_Artificial_Intelligence's_Imitative_Capabilities)  
10. College students are bombarded by misinformation, so this ... \- Reddit, accessed October 7, 2025, [https://www.reddit.com/r/highereducation/comments/1n0y557/college\_students\_are\_bombarded\_by\_misinformation/](https://www.reddit.com/r/highereducation/comments/1n0y557/college_students_are_bombarded_by_misinformation/)  
11. College students are bombarded by misinformation, so this professor taught them fact-checking 101 − here's what happened : r/Libraries \- Reddit, accessed October 7, 2025, [https://www.reddit.com/r/Libraries/comments/1n0y5om/college\_students\_are\_bombarded\_by\_misinformation/](https://www.reddit.com/r/Libraries/comments/1n0y5om/college_students_are_bombarded_by_misinformation/)  
12. An open interface to GDELT APIs \- GitHub, accessed October 7, 2025, [https://github.com/gdelt/gdelt.github.io](https://github.com/gdelt/gdelt.github.io)  
13. Event Registry \- NewsAPI.ai: Best News API & Leading News Analytics Solutions, accessed October 7, 2025, [https://newsapi.ai/about](https://newsapi.ai/about)  
14. Event Detection \- Dow Jones Developer Platform, accessed October 7, 2025, [https://developer.dowjones.com/solution-patterns/solution-patterns-details-event-detection](https://developer.dowjones.com/solution-patterns/solution-patterns-details-event-detection)  
15. Alternative data \- Enhancing accuracy in fraud detection, accessed October 7, 2025, [https://www.fraud.com/post/alternative-data](https://www.fraud.com/post/alternative-data)  
16. typpo/Public-APIs-1: A public list of APIs from round the web. \- GitHub, accessed October 7, 2025, [https://github.com/typpo/Public-APIs-1](https://github.com/typpo/Public-APIs-1)  
17. Bert Base NER · Models \- Dataloop, accessed October 7, 2025, [https://dataloop.ai/library/model/dslim\_bert-base-ner/](https://dataloop.ai/library/model/dslim_bert-base-ner/)  
18. Transformer based named entity recognition for place name extraction from unstructured text, accessed October 7, 2025, [https://www.tandfonline.com/doi/full/10.1080/13658816.2022.2133125](https://www.tandfonline.com/doi/full/10.1080/13658816.2022.2133125)  
19. Named-entity recognition \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Named-entity\_recognition](https://en.wikipedia.org/wiki/Named-entity_recognition)  
20. What is Named Entity Recognition (NER)? Methods, Use Cases, and Challenges, accessed October 7, 2025, [https://www.datacamp.com/blog/what-is-named-entity-recognition-ner](https://www.datacamp.com/blog/what-is-named-entity-recognition-ner)  
21. dslim/bert-base-NER \- Hugging Face, accessed October 7, 2025, [https://huggingface.co/dslim/bert-base-NER](https://huggingface.co/dslim/bert-base-NER)  
22. How to Do Named Entity Recognition (NER) with a BERT Model \- MachineLearningMastery.com, accessed October 7, 2025, [https://machinelearningmastery.com/how-to-do-named-entity-recognition-ner-with-a-bert-model/](https://machinelearningmastery.com/how-to-do-named-entity-recognition-ner-with-a-bert-model/)  
23. Comparative Study of Pre-Trained BERT and Large Language Models for Code-Mixed Named Entity Recognition \- arXiv, accessed October 7, 2025, [https://arxiv.org/html/2509.02514v1](https://arxiv.org/html/2509.02514v1)  
24. What is entity extraction? A beginner's guide \- Google Cloud, accessed October 7, 2025, [https://cloud.google.com/discover/what-is-entity-extraction](https://cloud.google.com/discover/what-is-entity-extraction)  
25. BERT Based Named Entity Recognition (NER) Tutorial And Demo \- Pragnakalp Techlabs, accessed October 7, 2025, [https://www.pragnakalp.com/bert-named-entity-recognition-ner-tutorial-demo/](https://www.pragnakalp.com/bert-named-entity-recognition-ner-tutorial-demo/)  
26. Understanding BERT with Huggingface Transformers NER \- Galileo AI, accessed October 7, 2025, [https://galileo.ai/blog/nlp-huggingface-transformers-ner-understanding-bert-with-galileo](https://galileo.ai/blog/nlp-huggingface-transformers-ner-understanding-bert-with-galileo)  
27. Token classification \- Hugging Face LLM Course, accessed October 7, 2025, [https://huggingface.co/learn/llm-course/chapter7/2](https://huggingface.co/learn/llm-course/chapter7/2)  
28. spaCy's NER model · spaCy Universe, accessed October 7, 2025, [https://spacy.io/universe/project/video-spacys-ner-model](https://spacy.io/universe/project/video-spacys-ner-model)  
29. Named Entity Recognition (NER) using spaCy, accessed October 7, 2025, [https://spacy.io/universe/project/video-spacys-ner-model-alt](https://spacy.io/universe/project/video-spacys-ner-model-alt)  
30. EntityRecognizer · spaCy API Documentation, accessed October 7, 2025, [https://spacy.io/api/entityrecognizer](https://spacy.io/api/entityrecognizer)  
31. Extracting locations from text using Python \- Tutorials Point, accessed October 7, 2025, [https://www.tutorialspoint.com/extracting-locations-from-text-using-python](https://www.tutorialspoint.com/extracting-locations-from-text-using-python)  
32. Natural Language Processing With spaCy in Python \- Real Python, accessed October 7, 2025, [https://realpython.com/natural-language-processing-spacy-python/](https://realpython.com/natural-language-processing-spacy-python/)  
33. Python | Named Entity Recognition (NER) using spaCy \- GeeksforGeeks, accessed October 7, 2025, [https://www.geeksforgeeks.org/python/python-named-entity-recognition-ner-using-spacy/](https://www.geeksforgeeks.org/python/python-named-entity-recognition-ner-using-spacy/)  
34. Training Pipelines & Models · spaCy Usage Documentation, accessed October 7, 2025, [https://spacy.io/usage/training](https://spacy.io/usage/training)  
35. Temporal Information Extraction \- Xiao Ling, accessed October 7, 2025, [https://xiaoling.github.io/pubs/ling-aaai10.pdf](https://xiaoling.github.io/pubs/ling-aaai10.pdf)  
36. A Survey on Temporal Reasoning for Temporal Information Extraction from Text, accessed October 7, 2025, [https://jair.org/index.php/jair/article/view/11727](https://jair.org/index.php/jair/article/view/11727)  
37. Survey of Temporal Information Extraction \- Korea Science, accessed October 7, 2025, [https://koreascience.kr/article/JAKO201925462478101.pdf](https://koreascience.kr/article/JAKO201925462478101.pdf)  
38. dateparser – python parser for human readable dates — DateParser 1.2.2 documentation, accessed October 7, 2025, [https://dateparser.readthedocs.io/](https://dateparser.readthedocs.io/)  
39. python parser for human readable dates — DateParser 0.5.1 documentation, accessed October 7, 2025, [https://dateparser.readthedocs.io/en/v0.5.1/](https://dateparser.readthedocs.io/en/v0.5.1/)  
40. dateparser \- PyPI, accessed October 7, 2025, [https://pypi.org/project/dateparser/](https://pypi.org/project/dateparser/)  
41. Relative and Incomplete Time Expression ... \- ACL Anthology, accessed October 7, 2025, [https://aclanthology.org/2020.clinicalnlp-1.14.pdf](https://aclanthology.org/2020.clinicalnlp-1.14.pdf)  
42. pyTLEX: A Python Library for TimeLine EXtraction \- ACL Anthology, accessed October 7, 2025, [https://aclanthology.org/2024.eacl-demo.4.pdf](https://aclanthology.org/2024.eacl-demo.4.pdf)  
43. What Is Event Extraction | Ontotext Fundamentals, accessed October 7, 2025, [https://www.ontotext.com/knowledgehub/fundamentals/what-is-event-extraction/](https://www.ontotext.com/knowledgehub/fundamentals/what-is-event-extraction/)  
44. Ultimate guide to the spaCy library in Python \- Deepnote, accessed October 7, 2025, [https://deepnote.com/blog/ultimate-guide-to-the-spacy-library-in-python](https://deepnote.com/blog/ultimate-guide-to-the-spacy-library-in-python)  
45. Geocoding Addresses Best Practices | Geocoding API | Google for ..., accessed October 7, 2025, [https://developers.google.com/maps/documentation/geocoding/best-practices](https://developers.google.com/maps/documentation/geocoding/best-practices)  
46. Maps Geocoding FAQ \- Salesforce Help, accessed October 7, 2025, [https://help.salesforce.com/s/articleView?id=000380853\&language=en\_US\&type=1](https://help.salesforce.com/s/articleView?id=000380853&language=en_US&type=1)  
47. Best Practices Using Geocoding API Web Services \- Google for Developers, accessed October 7, 2025, [https://developers.google.com/maps/documentation/geocoding/web-service-best-practices](https://developers.google.com/maps/documentation/geocoding/web-service-best-practices)  
48. Knowledge Graph: Powering intelligent and context-aware search ..., accessed October 7, 2025, [https://cloud.google.com/agentspace/docs/use-knowledge-graph-search](https://cloud.google.com/agentspace/docs/use-knowledge-graph-search)  
49. Giving Context to Knowledge Graphs: The Context Engine Playbook for Real-World AI and Analytics \- \- BIX Tech, accessed October 7, 2025, [https://bix-tech.com/giving-context-to-knowledge-graphs-the-context-engine-playbook-for-real-world-ai-and-analytics/](https://bix-tech.com/giving-context-to-knowledge-graphs-the-context-engine-playbook-for-real-world-ai-and-analytics/)  
50. Context-Aware Explainable Recommendation Based on Domain Knowledge Graph \- MDPI, accessed October 7, 2025, [https://www.mdpi.com/2504-2289/6/1/11](https://www.mdpi.com/2504-2289/6/1/11)  
51. FED: Fast and Efficient Dataset Deduplication Framework with GPU Acceleration \- arXiv, accessed October 7, 2025, [https://arxiv.org/html/2501.01046v2](https://arxiv.org/html/2501.01046v2)  
52. Effective Data Deduplication for Training Robust Language Models \- Python in Plain English, accessed October 7, 2025, [https://python.plainenglish.io/effective-data-deduplication-for-training-robust-language-models-44467afac5bb](https://python.plainenglish.io/effective-data-deduplication-for-training-robust-language-models-44467afac5bb)  
53. The Ultimate Data Deduplication Guide \- WinPure, accessed October 7, 2025, [https://winpure.com/data-deduplication-guide/](https://winpure.com/data-deduplication-guide/)  
54. Near-duplicate Detection with Locality-Sensitive Hashing and Datasketch \- Yury Kashnitsky, accessed October 7, 2025, [https://yorko.github.io/2023/practical-near-dup-detection/](https://yorko.github.io/2023/practical-near-dup-detection/)  
55. Minhash LSH Implementation Walkthrough: Deduplication \- DZone, accessed October 7, 2025, [https://dzone.com/articles/minhash-lsh-implementation-walkthrough](https://dzone.com/articles/minhash-lsh-implementation-walkthrough)  
56. MinHash LSH — datasketch 1.6.5 documentation, accessed October 7, 2025, [https://ekzhu.com/datasketch/lsh.html](https://ekzhu.com/datasketch/lsh.html)  
57. API Documentation \- datasketch.MinHash, accessed October 7, 2025, [http://ekzhu.com/datasketch/documentation.html](http://ekzhu.com/datasketch/documentation.html)  
58. Deduplication Deduplication. Deduplication of text is an application… | by Abhijith C | TDS Archive | Medium, accessed October 7, 2025, [https://medium.com/data-science/deduplication-deduplication-1d1414ffb4d2](https://medium.com/data-science/deduplication-deduplication-1d1414ffb4d2)  
59. Semantic deduplication of redundant and non-conformant data in temporal domains \- LOUIS, accessed October 7, 2025, [https://louis.uah.edu/uah-dissertations/277/](https://louis.uah.edu/uah-dissertations/277/)  
60. Sentence-BERT for Similarity Matching: Find and Rank Results \- ThatWare, accessed October 7, 2025, [https://thatware.co/sentence-bert-for-similarity-matching/](https://thatware.co/sentence-bert-for-similarity-matching/)  
61. Sentence Similarity using BERT Transformer \- GeeksforGeeks, accessed October 7, 2025, [https://www.geeksforgeeks.org/nlp/sentence-similarity-using-bert-transformer/](https://www.geeksforgeeks.org/nlp/sentence-similarity-using-bert-transformer/)  
62. Semantic Textual Similarity — Sentence Transformers documentation, accessed October 7, 2025, [https://sbert.net/docs/sentence\_transformer/usage/semantic\_textual\_similarity.html](https://sbert.net/docs/sentence_transformer/usage/semantic_textual_similarity.html)  
63. Semantic Search — Sentence Transformers documentation, accessed October 7, 2025, [https://www.sbert.net/examples/sentence\_transformer/applications/semantic-search/README.html](https://www.sbert.net/examples/sentence_transformer/applications/semantic-search/README.html)  
64. How to Implement Semantic Search in Python Step by Step \- TiDB, accessed October 7, 2025, [https://www.pingcap.com/article/semantic-search-python-step-by-step/](https://www.pingcap.com/article/semantic-search-python-step-by-step/)  
65. Top 4 Sentence Embedding Techniques using Python \- Analytics Vidhya, accessed October 7, 2025, [https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/](https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/)  
66. Faiss | 🦜️ LangChain, accessed October 7, 2025, [https://python.langchain.com/docs/integrations/vectorstores/faiss/](https://python.langchain.com/docs/integrations/vectorstores/faiss/)  
67. facebookresearch/faiss: A library for efficient similarity search and clustering of dense vectors. \- GitHub, accessed October 7, 2025, [https://github.com/facebookresearch/faiss](https://github.com/facebookresearch/faiss)  
68. Scaling Semantic Search with FAISS: Challenges and Solutions for Billion-Scale Datasets | by Devesh Bajaj | Medium, accessed October 7, 2025, [https://medium.com/@deveshbajaj59/scaling-semantic-search-with-faiss-challenges-and-solutions-for-billion-scale-datasets-1cacb6f87f95](https://medium.com/@deveshbajaj59/scaling-semantic-search-with-faiss-challenges-and-solutions-for-billion-scale-datasets-1cacb6f87f95)  
69. Introduction to Facebook AI Similarity Search (Faiss) \- Pinecone, accessed October 7, 2025, [https://www.pinecone.io/learn/series/faiss/faiss-tutorial/](https://www.pinecone.io/learn/series/faiss/faiss-tutorial/)  
70. FAISS and sentence-transformers in 5 Minutes \- Stephen Diehl, accessed October 7, 2025, [https://www.stephendiehl.com/posts/faiss/](https://www.stephendiehl.com/posts/faiss/)  
71. Building a Fast Semantic Search Engine with Python, FAISS, and Sentence Transformers | by shreeraman karikalan | Medium, accessed October 7, 2025, [https://medium.com/@shreeraman-ak/building-a-fast-semantic-search-engine-with-python-faiss-and-sentence-transformers-e3df7c47340c](https://medium.com/@shreeraman-ak/building-a-fast-semantic-search-engine-with-python-faiss-and-sentence-transformers-e3df7c47340c)  
72. Mastering Semantic Search with FAISS and Langchain \- MyScale, accessed October 7, 2025, [https://myscale.com/blog/mastering-semantic-search-faiss-langchain-step-by-step-guide/](https://myscale.com/blog/mastering-semantic-search-faiss-langchain-step-by-step-guide/)  
73. Tutorial: semantic search using Faiss & MPNet \- Deepnote, accessed October 7, 2025, [https://deepnote.com/blog/semantic-search-using-faiss-and-mpnet](https://deepnote.com/blog/semantic-search-using-faiss-and-mpnet)  
74. Semantic search with FAISS \- Hugging Face LLM Course, accessed October 7, 2025, [https://huggingface.co/learn/llm-course/chapter5/6](https://huggingface.co/learn/llm-course/chapter5/6)  
75. FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication \- CVF Open Access, accessed October 7, 2025, [https://openaccess.thecvf.com/content/CVPR2024/papers/Slyman\_FairDeDup\_Detecting\_and\_Mitigating\_Vision-Language\_Fairness\_Disparities\_in\_Semantic\_Dataset\_CVPR\_2024\_paper.pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Slyman_FairDeDup_Detecting_and_Mitigating_Vision-Language_Fairness_Disparities_in_Semantic_Dataset_CVPR_2024_paper.pdf)  
76. The Ultimate Guide to FAISS Indexing with Sentence Transformers for Semantic Search, accessed October 7, 2025, [https://www.aitude.com/the-ultimate-guide-to-faiss-indexing-with-sentence-transformers-for-semantic-search/](https://www.aitude.com/the-ultimate-guide-to-faiss-indexing-with-sentence-transformers-for-semantic-search/)  
77. FAISS: A quick tutorial to efficient similarity search | by Shayan Fazeli \- Medium, accessed October 7, 2025, [https://shayan-fazeli.medium.com/faiss-a-quick-tutorial-to-efficient-similarity-search-595850e08473](https://shayan-fazeli.medium.com/faiss-a-quick-tutorial-to-efficient-similarity-search-595850e08473)  
78. Data Pipeline Architecture: Diagrams, Best Practices, and Examples ..., accessed October 7, 2025, [https://airbyte.com/data-engineering-resources/data-pipeline-architecture](https://airbyte.com/data-engineering-resources/data-pipeline-architecture)  
79. ETL/ELT | Apache Airflow, accessed October 7, 2025, [https://airflow.apache.org/use-cases/etl\_analytics/](https://airflow.apache.org/use-cases/etl_analytics/)  
80. A complete Apache Airflow tutorial: building data pipelines with Python \- AI Summer, accessed October 7, 2025, [https://theaisummer.com/apache-airflow-tutorial/](https://theaisummer.com/apache-airflow-tutorial/)  
81. anastasiia-p/airflow-ml: Airflow Pipeline for Machine Learning \- GitHub, accessed October 7, 2025, [https://github.com/anastasiia-p/airflow-ml](https://github.com/anastasiia-p/airflow-ml)  
82. Building a Simple Data Pipeline \- Apache Airflow, accessed October 7, 2025, [https://airflow.apache.org/docs/apache-airflow/stable/tutorial/pipeline.html](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/pipeline.html)  
83. End-to-End Data Pipeline on GCP with Airflow: A Social Media Case Study Part 2, accessed October 7, 2025, [https://randomtrees.medium.com/end-to-end-data-pipeline-on-gcp-with-airflow-a-social-media-case-study-part-2-7b9987c343d0](https://randomtrees.medium.com/end-to-end-data-pipeline-on-gcp-with-airflow-a-social-media-case-study-part-2-7b9987c343d0)  
84. Build an ML Inference Data Pipeline using SageMaker and Apache Airflow \- Medium, accessed October 7, 2025, [https://medium.com/mlearning-ai/accelerate-ml-inference-data-pipeline-using-sagemaker-and-apache-airflow-f19207e896ca](https://medium.com/mlearning-ai/accelerate-ml-inference-data-pipeline-using-sagemaker-and-apache-airflow-f19207e896ca)  
85. Dina-Hosny/ETL-Data-Pipeline-using-AirFlow \- GitHub, accessed October 7, 2025, [https://github.com/Dina-Hosny/ETL-Data-Pipeline-using-AirFlow](https://github.com/Dina-Hosny/ETL-Data-Pipeline-using-AirFlow)  
86. Geo Point data in ElasticSearch/Kibana from Python app \- Stack Overflow, accessed October 7, 2025, [https://stackoverflow.com/questions/40294413/geo-point-data-in-elasticsearch-kibana-from-python-app](https://stackoverflow.com/questions/40294413/geo-point-data-in-elasticsearch-kibana-from-python-app)  
87. Geo queries | Reference \- Elastic, accessed October 7, 2025, [https://www.elastic.co/docs/reference/query-languages/query-dsl/geo-queries](https://www.elastic.co/docs/reference/query-languages/query-dsl/geo-queries)  
88. Elasticsearch geospatial search with ES|QL, accessed October 7, 2025, [https://www.elastic.co/search-labs/blog/esql-geospatial-search-part-one](https://www.elastic.co/search-labs/blog/esql-geospatial-search-part-one)  
89. Elasticsearch Geo Distance Query \- Syntax, Example, and Tips \- Pulse, accessed October 7, 2025, [https://pulse.support/kb/elasticsearch-geo-distance-query](https://pulse.support/kb/elasticsearch-geo-distance-query)  
90. Python Elasticsearch client 9.1.1 documentation, accessed October 7, 2025, [https://elasticsearch-py.readthedocs.io/en/latest/api/elasticsearch.html](https://elasticsearch-py.readthedocs.io/en/latest/api/elasticsearch.html)