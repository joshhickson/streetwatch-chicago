# **Architecting a Real-Time Geospatial Intelligence Pipeline: An Automated System for Mapping ICE and CBP Activity in Chicago**

## **Section 1: Architectural Blueprint for the Automated Mapping System**

This section establishes a comprehensive, high-level architectural framework for the automated mapping system. It presents a master schematic that delineates the complete data lifecycle, from its initial state as unstructured text on social media platforms to its final representation as a structured, interactive geospatial visualization. The system is designed with a modular approach, emphasizing a clear separation of concerns across distinct processing stages. This architectural choice is fundamental to enhancing the system's maintainability, scalability, and resilience, allowing for independent development and optimization of each component.

### **1.1. System Overview and Core Principles**

The primary objective of this project is to engineer a sophisticated tool for civic awareness by systematically collecting, processing, and visualizing publicly available, user-generated information regarding law enforcement activity within the city of Chicago. The system is designed to operate as a continuous data pipeline, transforming anecdotal reports into a structured, queryable, and geographically referenced dataset.  
The system's architecture is segmented into four primary, logically distinct stages, each responsible for a specific phase of the data transformation process:

1. **Ingestion:** This initial stage is responsible for the automated monitoring and retrieval of potentially relevant data from designated online sources, specifically the social media platform Reddit. It acts as the system's sensor, continuously scanning for new information that matches predefined criteria.  
2. **Processing & Enrichment:** Once raw data is ingested, this stage executes the core intelligence-gathering functions. It involves advanced Natural Language Processing (NLP) to extract key entities such as locations and times from unstructured text. Subsequently, it enriches this extracted information by converting textual location descriptions into precise geographic coordinates (geocoding) and aggregating supplementary data, such as source URLs and contextual descriptions.  
3. **Storage & Formatting:** This stage takes the processed and enriched data and structures it into a standardized, machine-readable format. The output is a data artifact, specifically a Comma-Separated Values (CSV) file, meticulously formatted to be compatible with the chosen visualization platform. This stage ensures data integrity and prepares it for its final presentation.  
4. **Visualization:** The final stage involves rendering the structured data as interactive points on a custom map. This is the user-facing component of the system, where the aggregated intelligence is made accessible and interpretable through a geospatial interface.

The technological philosophy underpinning this architecture is both "cloud-native" and "AI-augmented." A cloud-native approach is adopted by leveraging serverless and managed platforms, such as Replit for code execution and n8n Cloud for workflow automation. This strategy deliberately minimizes the operational burden of infrastructure management, allowing development efforts to focus on the core logic of the data pipeline rather than on server provisioning, scaling, and maintenance. Concurrently, the system is AI-augmented, integrating advanced AI coding agents like Google Jules. This modern development practice, sometimes referred to as "vibecoding," accelerates the software development lifecycle by delegating the generation of boilerplate code, API integrations, and unit tests to an AI, enabling the human developer to operate at a higher level of abstraction, focusing on system design and logical intent.

### **1.2. Detailed Data Flow Diagram**

The following diagram illustrates the sequential flow of data and processing steps within the architecture. It provides a visual representation of the journey from raw data source to final visualization, highlighting the interactions between the various technological components.

* **Trigger:** The workflow is initiated by a Schedule node within the n8n automation platform. This trigger activates the entire pipeline at a predefined, recurring interval (e.g., every 15 minutes), ensuring consistent and timely data collection.  
* **Step 1 (Ingestion):** Upon triggering, an n8n node executes a Search Posts operation against the Reddit API. The query is configured to search specific subreddits (e.g., r/chicago) for posts containing a predefined set of keywords (e.g., ICE, CBP, police). The search is time-bound to fetch only new posts created since the workflow's last execution.  
* **Step 2 (Filtering):** The initial results from the Reddit API are passed through a filtering node within n8n. This step performs a preliminary cleansing of the data, discarding posts that are clearly irrelevant based on simple rules, such as the presence of negative keywords or specific post flairs.  
* **Step 3 (Processing Hand-off):** For each post that passes the initial filter, the n8n workflow constructs and sends an HTTP POST request to a dedicated API endpoint. This endpoint is hosted on a Python web application running in a Replit environment. The request payload contains the relevant data from the Reddit post, primarily its title, body text, timestamp, and permalink.  
* **Step 4 (Natural Language Processing \- NLP):** The Python script running on Replit receives the incoming request. It executes a custom-trained spaCy Named Entity Recognition (NER) model on the post's text. This specialized model is designed to identify and extract Chicago-specific location entities, such as cross-street intersections, neighborhoods, and specific addresses.  
* **Step 5 (Data Enrichment \- Geocoding):** The script takes the location strings extracted by the NER model and makes a series of API calls to the Google Geocoding API. This service converts the human-readable location text into precise geographic coordinates (latitude and longitude).  
* **Step 6 (Data Hygiene \- Deduplication):** Before finalizing the new data point, the script performs a deduplication check. It compares the timestamp and coordinates of the new event against a cache of recently processed events. If the new event is within a predefined proximity in both time and space to an existing entry, it is classified as a duplicate and discarded, preventing redundant markers on the map.  
* **Step 7 (Data Formatting):** The script assembles a complete, structured data record. This record includes the geocoded coordinates, the original timestamp from the Reddit post, the source URL, a concise description of the event, and any links to external media (e.g., videos) found in the post. This record is formatted as a new row destined for a CSV file.  
* **Step 8 (Storage and Versioning):** The Python script programmatically updates a master CSV file (map\_data.csv) hosted within a GitHub repository. It commits the changes, creating a new version of the data file. This use of GitHub provides a robust, version-controlled, and auditable history of the entire dataset.  
* **Step 9 (Visualization \- The Manual Air Gap):** The final step in the process is the only one that requires human intervention. The user periodically navigates to the Google MyMaps interface and performs a data import operation. They refresh the map's data layer by uploading the latest version of the map\_data.csv file from the GitHub repository. This diagram explicitly labels this step as the "manual air gap," acknowledging the platform's limitations.

### **1.3. Analysis of the Core Technical Challenge: The MyMaps API Limitation**

A critical factor shaping this entire architecture is a fundamental constraint of the chosen visualization platform: Google MyMaps. Extensive investigation across developer communities, official documentation, and public issue trackers confirms the absence of a public, writable Application Programming Interface (API) for Google MyMaps. This means it is not possible to programmatically add, delete, or update individual map points in an automated, real-time fashion. Any solution that claims full, end-to-end automation with Google MyMaps as the final destination is technically infeasible.  
This constraint necessitates a strategic pivot in the architectural design. Instead of a tightly coupled, fully integrated system where the automation engine directly manipulates the map, the architecture is designed as a sophisticated data pipeline that produces a high-quality, consumable data artifact. The workflow fully automates every stage of the process up to and including the generation of a perfectly formatted CSV file. This file is structured precisely to align with the import requirements of Google MyMaps.  
The resulting process is therefore semi-automated, built around a "human-in-the-loop" model for the final visualization step. The manual part of the workflow—deleting the old map layer and importing the new CSV file—is a simple, repeatable action that takes only a few clicks. This approach preserves the immense value and complexity of the automated data processing pipeline while pragmatically working within the defined limitations of the target platform. This "manual air gap" is not a system failure but a deliberate and necessary design choice that ensures reliability and functionality.  
For future development, a pathway to full automation exists through the adoption of a more developer-centric tool, the Google Maps JavaScript API. This alternative would involve building a custom web application to host and render the map. While this requires significantly more development effort, it would provide complete programmatic control, allowing for real-time updates directly from the data pipeline. This option is further explored as a potential enhancement in Section 6 of this report. This architectural decision elevates the role of GitHub beyond a simple code repository. It becomes the canonical, version-controlled data store for the project's entire set of geospatial intelligence points.

## **Section 2: The Automation Engine: A Comparative Analysis of n8n and Node-RED**

This section provides an in-depth analysis of the core automation platform, which serves as the orchestrator for the entire data ingestion and processing pipeline. A rigorous comparison between the two specified tools, n8n and Node-RED, is conducted to justify the selection of the optimal platform for this specific use case. Following the tool selection, a detailed guide is provided for configuring the necessary API connections and constructing the data ingestion workflow.

### **2.1. Tool Selection: n8n as the Superior Choice**

The selection of the automation engine is a critical architectural decision. The evaluation of n8n and Node-RED is based on a framework of criteria directly derived from the project's core requirements: API-centric orchestration, ease of integration with modern web services, and alignment with an AI-augmented development philosophy.  
n8n demonstrates clear superiority for this project due to its fundamental design principles. It is a platform purpose-built for connecting disparate cloud services and APIs, which is the central task of this workflow. The entire system relies on orchestrating calls between the Reddit API, a custom API endpoint on Replit, and the Google Geocoding API. n8n's architecture is a natural and efficient fit for this model. Furthermore, it offers a comprehensive library of over 1,000 pre-built integration nodes, including a robust and well-maintained node specifically for Reddit. This significantly reduces development time and complexity compared to implementing raw API calls. Finally, n8n exhibits stronger and more contemporary integrations with AI services, as evidenced by numerous community templates that leverage platforms like OpenAI for tasks such as summarization and classification. This aligns perfectly with the project's AI-augmented approach.  
Conversely, Node-RED, while a powerful and versatile tool, is optimized for a different class of problems. Its core strengths lie in event-driven applications for the Internet of Things (IoT), hardware integration, and local network automation. These capabilities are not relevant to this project's requirements. While Node-RED possesses excellent UI and dashboarding features, these are intended for creating user-facing interfaces, which is not the function of this backend data processing workflow.  
Based on this direct comparison, n8n is the unequivocally recommended choice. Its focus on API-driven workflows, extensive library of pre-built connectors, and strong AI integration capabilities make it the ideal engine to power this geospatial intelligence pipeline.

| Feature/Criterion | n8n | Node-RED | Recommendation for this Project |
| :---- | :---- | :---- | :---- |
| **Primary Use Case** | Cloud Service & API Orchestration | IoT, Edge Computing, Local Workflows | n8n |
| **Reddit Integration** | Native, well-supported node | Community-contributed nodes | n8n |
| **AI/LLM Integration** | Strong, with built-in support and templates | Possible via custom functions or community nodes | n8n |
| **Hosting Model** | Cloud or Self-hosted | Primarily Self-hosted | n8n (Cloud for ease of use) |
| **Learning Curve** | Generally considered more user-friendly for API workflows | Steeper for complex, non-IoT tasks | n8n |
| **User Interface** | Modern, focused on linear workflows | Visual flow editor, strong in dashboarding | n8n |

### **2.2. Configuring the Reddit API Connection in n8n**

For any meaningful and reliable interaction, the Reddit API mandates the use of OAuth2 authentication. Unauthenticated requests are subject to extremely restrictive rate limits and are often blocked entirely, making them unsuitable for any production application. The following steps detail the process for establishing a secure and authorized connection within n8n.

1. **Create a Reddit Application:** The first step is to register a new application within the Reddit user account that will be used for the automation. This is done by navigating to the "apps" page in Reddit's preferences. A new application must be created, and critically, it must be designated as a "script" type app. During this process, Reddit will generate a unique Client ID and a Client Secret. These two values are the essential credentials for the OAuth2 authentication flow.  
2. **Configure n8n Credentials:** Within the n8n interface, navigate to the "Credentials" section and create a new credential for the Reddit service. Select "OAuth2" as the authentication method. In the configuration dialog, paste the Client ID and Client Secret obtained from the Reddit app page. n8n will provide a "Redirect URI" that must be copied and pasted back into the corresponding field in the Reddit app's settings. Once configured, n8n will guide the user through a one-time authorization flow to link the credential to the Reddit account.  
3. **Adhere to User-Agent Policy:** Reddit enforces a strict policy regarding the User-Agent header sent with every API request. Clients must identify themselves with a unique and descriptive string to avoid being throttled or blocked. The recommended format is \<platform\>:\<app ID\>:\<version string\> (by /u/\<reddit username\>). While n8n's native Reddit node likely handles this requirement automatically by constructing a compliant header, it is a critical policy to be aware of, especially if using the generic HTTP Request node for more advanced operations.

### **2.3. Building the Ingestion Workflow: Monitoring Subreddits**

The ingestion workflow is the heart of the data collection process. It is responsible for identifying and retrieving relevant posts from Reddit in a timely and efficient manner.  
The primary data source will be the r/chicago subreddit, which has been identified as a hub for discussions on local events, including law enforcement activity and ICE sightings. The system will also be configured to monitor other relevant communities, such as r/AskChicago, to broaden the scope of data collection. The workflow will search for posts containing a curated list of keywords, including but not limited to: ice, cbp, border patrol, police, raid, and checkpoint.  
A key architectural consideration is the implementation of the triggering mechanism. The native n8n Reddit node is action-based, providing operations like Get and Search, but it does not function as a "trigger" that automatically activates a workflow when a new post is created. Consequently, a true, real-time, push-based trigger is not available out-of-the-box. The optimal technical solution is to simulate this functionality through scheduled polling. The workflow will be initiated by a Schedule (or Cron) node, which will activate the entire process at a regular interval. Upon activation, the workflow will use the Reddit node's Search Posts operation to query for posts that match the specified keywords and have been created since the last time the workflow was executed. This pull-based model effectively and reliably emulates a "new post" trigger.  
This polling strategy must be designed to operate within the constraints of Reddit's API rate limits. Authenticated clients are permitted up to 100 queries per minute (QPM). A polling interval of once every 10 to 15 minutes is extremely conservative, placing the system's usage well below the API limits and eliminating any risk of rate-limiting or throttling. This frequency is more than sufficient to provide near-real-time data for the intended use case. For added resilience, the workflow should be designed to monitor the rate-limiting headers returned by the Reddit API (X-Ratelimit-Used, X-Ratelimit-Remaining) to enable intelligent backoff and error handling in the unlikely event of approaching the limit.

## **Section 3: The Development & AI Augmentation Framework**

This section outlines the modern, cloud-native development environment where the system's core intellectual property—the custom Python script for Natural Language Processing—will be conceived, developed, managed, and hosted. The framework is built upon a triad of integrated tools: Replit for the development and hosting environment, GitHub for version control and data storage, and Google Jules as an AI-powered coding agent to accelerate the development process.

### **3.1. Replit: The Cloud-Native Integrated Development Environment (IDE)**

Replit is selected as the IDE and execution environment for this project due to its zero-configuration, browser-based nature. This platform entirely eliminates the complexities traditionally associated with setting up and maintaining a local development environment, such as managing Python versions, virtual environments, and system dependencies. For this project, Replit's most crucial feature is its ability to instantly deploy a web application (in this case, a simple Flask API) and provide a publicly accessible URL endpoint. This endpoint is the critical bridge that allows the n8n workflow to hand off data to the Python script for processing via a simple HTTP request.  
A key advantage of Replit is its seamless, two-way integration with GitHub. The development process begins by importing the project's GitHub repository directly into a new Replit workspace. Code can then be written, tested, and debugged within Replit's collaborative, real-time environment. Once changes are finalized, Replit's integrated Git interface allows the developer to stage, commit, and push the new code directly back to the GitHub repository. This creates a fluid and efficient development loop, combining the benefits of a managed cloud IDE with the robustness of a distributed version control system.

### **3.2. GitHub: The Central Hub for Code and Data**

GitHub serves as the foundational pillar of the project, acting as the single source of truth for both the application's code and its generated data. Its primary role is to provide robust version control for all project assets. This includes the Python scripts for NLP and geocoding, the spaCy model configuration files, the annotated training data for the custom NER model, and any other supporting code. By leveraging Git, the project gains a complete, auditable history of every change, enabling collaboration, issue tracking, and the ability to roll back to previous versions if necessary.  
As established in the architectural blueprint, GitHub's role extends beyond code management. The repository will also host the final data artifact: the map\_data.csv file. Each time the automated workflow successfully processes a new sighting, the updated CSV file is committed to the repository. This innovative use of GitHub transforms it into a version-controlled, transactional data store. It provides a transparent and auditable log of every point of interest added to the map, ensuring data provenance and integrity.

### **3.3. Google Jules & "Vibecoding": Accelerating Development with AI**

To significantly accelerate the development of the core Python application, the project will leverage Google Jules, an advanced, asynchronous AI coding agent. Unlike traditional code completion tools that suggest single lines or blocks of code, Jules operates as an autonomous agent capable of understanding and executing complex, high-level tasks. The developer provides Jules with a natural language prompt describing the desired functionality, and Jules clones the repository into a secure cloud VM, performs the work, writes the code, adds dependencies, and even generates tests before submitting a pull request for human review.  
This methodology embodies the concept of "vibecoding," a paradigm shift in software development where the human engineer focuses on the architectural intent, the logical flow, and the overall "vibe" of the application, while the AI agent handles the detailed, often tedious, implementation work. This collaborative approach allows for rapid prototyping and development.  
The practical application of Jules in this project would follow a sequence of prompts:

1. **Initial Project Scaffolding:** A developer could initiate the project with a prompt to Jules such as: *"In this new GitHub repository, create a Python project using the Flask framework. The project should have a single API endpoint at /process-sighting that accepts HTTP POST requests. The endpoint should be able to parse an incoming JSON payload that contains a field named 'post\_text'."*  
2. **Developing the NLP Core:** The next prompt would build upon this foundation: *"Integrate the spaCy library into the project. Within the /process-sighting endpoint, load the 'en\_core\_web\_sm' model. Use this model to perform Named Entity Recognition on the 'post\_text' from the JSON payload. The endpoint should return a JSON object containing a list of all extracted entities that have the label 'GPE' or 'LOC'."*  
3. **Integrating the Geocoding Service:** Finally, the developer would instruct Jules to add the data enrichment layer: *"Create a new function that accepts a location string as input. This function should use the Python requests library to make an API call to the Google Geocoding API and return the corresponding latitude and longitude. Ensure there is robust error handling for cases where the API call fails or returns no results. Integrate this function into the /process-sighting endpoint."*

This entire process can be managed and scripted using Jules Tools, the command-line interface (CLI) for the Jules agent. The CLI can be run directly from the terminal within the Replit environment, creating a powerful and tightly integrated workflow where the developer can spin up, inspect, and steer AI-driven development tasks without ever leaving their IDE. This combination of Replit, GitHub, and Jules forms a self-contained and highly efficient development triad, abstracting away traditional infrastructure and dramatically shortening the path from concept to functional, production-ready code.

## **Section 4: Natural Language Processing for Location Intelligence**

This section addresses the technical core of the project: the transformation of unstructured, human-generated text from Reddit posts into structured, machine-readable geographic data. It details the selection of the appropriate Natural Language Processing (NLP) library, outlines the critical need for a custom-trained model to handle domain-specific language, provides a practical guide to the model training process, and describes the final step of converting extracted location strings into precise map coordinates.

### **4.1. NLP Library Selection: spaCy over NLTK**

The choice of a Python NLP library is a foundational decision for the data processing component. While several powerful libraries exist, including the Natural Language Toolkit (NLTK) and TextBlob, spaCy is the recommended choice for this project.  
NLTK is a highly respected and comprehensive library, often favored in academic and research settings. It provides a vast array of tools and algorithms, offering granular control over every step of the NLP pipeline. However, this flexibility comes at the cost of increased complexity; it requires the developer to manually assemble these building blocks to create a functional system.  
In contrast, spaCy is a modern, opinionated library specifically designed for building production-grade NLP applications. It offers state-of-the-art, pre-trained statistical models that are fast, efficient, and easy to use. Its streamlined API for common tasks, particularly Named Entity Recognition (NER), and its well-documented process for training custom models, make it the ideal tool for this project's requirements. spaCy's focus on performance and developer experience in a production context gives it a distinct advantage over more research-oriented libraries like NLTK.

### **4.2. The Limits of Pre-Trained Models and the Need for Custom NER**

While spaCy's pre-trained models are remarkably powerful, they have inherent limitations that make them insufficient for this project's specific needs. These off-the-shelf models are trained on large, general-purpose corpora like Wikipedia and web text. As a result, they excel at identifying standard, well-defined entity types, such as GPE (Geopolitical Entity), which includes cities, states, and countries (e.g., "Chicago," "Illinois"), and LOC (Location), which includes non-geopolitical places like mountain ranges or bodies of water (e.g., "Lake Michigan").  
The critical challenge for this project—the "cross-streets problem"—lies in identifying informal, colloquial, and highly specific location formats commonly used in user-generated content. A pre-trained model will not recognize phrases like "corner of Clark and Division," "Milwaukee and Damen," "ICE near the Howard Red Line stop," or local neighborhood nicknames as location entities. These are the most valuable pieces of information in the source data, and the standard models will fail to extract them.  
Therefore, the only robust and reliable solution is to train a custom NER model. This involves creating a new model or fine-tuning an existing one on a bespoke dataset composed of examples of these Chicago-specific location phrases. A new, custom entity label, such as CHI\_LOCATION, will be created to specifically identify these valuable data points. This custom model is the "secret sauce" of the entire system, providing the specialized intelligence that a general-purpose tool cannot.

### **4.3. A Practical Guide to Training a Custom spaCy NER Model**

The process of training a custom NER model in spaCy is a well-defined data science task. The following steps provide a practical, high-level guide to building the specialized model required for this project.

1. **Data Annotation:** The most critical and labor-intensive step is the creation of a high-quality training dataset. This involves manually collecting a representative sample of sentences from the target subreddits that contain the types of location phrases the model needs to learn. Each of these phrases must then be meticulously labeled with its start and end character indices and the custom entity type. For example, the sentence "I saw three CBP vans at the corner of Clark and Division" would be annotated in a format like: {"text": "I saw three CBP vans at the corner of Clark and Division", "labels":\]}. The quality and quantity of this annotated data will directly determine the final model's accuracy.  
2. **Data Conversion:** Once a sufficient amount of data has been annotated (typically a few hundred examples to start), it must be converted into spaCy's efficient binary .spacy format. This is accomplished using a helper script provided in the spaCy documentation, which takes the annotated JSON data as input and produces the binary training file.  
3. **Configuration:** The training process is controlled by a configuration file, typically named config.cfg. This file specifies all aspects of the model, including the base language model to start from (e.g., en\_core\_web\_sm), the components in the processing pipeline (e.g., ner), the model architecture, and various hyperparameters that control the training process (e.g., learning rate, number of iterations). spaCy provides tools to generate a base configuration file that can then be customized for the specific task.  
4. **Training:** With the configuration file and the binary training data prepared, the training process is initiated with a single command from the terminal within the Replit environment: python \-m spacy train config.cfg \--output./output. spaCy will then begin the iterative training process, adjusting the model's weights to minimize the difference between its predictions and the labeled examples.  
5. **Evaluation and Iteration:** Upon completion, spaCy automatically evaluates the trained model against a separate set of test data and reports its performance using standard metrics: precision (the accuracy of its positive predictions), recall (its ability to find all relevant entities), and the F1-score (the harmonic mean of precision and recall). The model can be iteratively improved by annotating more data, especially examples where it currently makes mistakes, and repeating the training process.  
6. **Integration:** The final output of the training process is a directory containing the fully trained and packaged custom model. This model can then be loaded and used by the main Python script in Replit with a simple command: nlp \= spacy.load("./output/model-best").

### **4.4. Geocoding: Converting Text to Coordinates**

After the custom NER model successfully extracts a location string like "Clark and Division," the final processing step is to convert this text into geographic coordinates (latitude and longitude) that can be plotted on a map. The Google Geocoding API is the industry-standard service for this task, offering high accuracy and comprehensive global data.  
The Python script will make an API request to the Geocoding service for each extracted location string. The request includes the address string itself and a unique API key for authentication and billing. The API returns a detailed JSON object containing a list of potential matches. For a query like "Clark and Division, Chicago, IL," the response will include the precise latitude and longitude coordinates, along with other information like the formatted address and the type of location match (e.g., GEOMETRIC\_CENTER for an intersection).  
The script must be designed to handle potential ambiguities. The API may return multiple results, so the logic should be programmed to select the most probable match, which is typically the first result in the list. To further improve accuracy and reduce ambiguity, all geocoding requests should be programmatically biased to the Chicago metropolitan area by including region parameters in the API call.  
Proper management of the API key is essential for security and cost control. A Google Cloud Platform project must be created, the Geocoding API must be enabled, and an API key must be generated. This key should be restricted to accept requests only from the Replit application's domain or IP address. Crucially, the API key must be stored as a secure environment variable (a "secret") within the Replit environment and should never be hardcoded directly into the Python script.

## **Section 5: The Visualization Layer: Mastering Google MyMaps and Its Automation Constraints**

This section is dedicated to the final stage of the data pipeline: the visualization of processed intelligence on a custom map. It details the capabilities of Google MyMaps as the chosen platform, defines the precise data structure required for successful data import, addresses the technical nuances of incorporating rich media like images and videos, and provides a clear, actionable guide for the manual update process that bridges the gap left by the platform's lack of a writable API.

### **5.1. Google MyMaps as the Visualization Platform**

Google MyMaps is an exceptionally user-friendly tool for creating and sharing custom maps, making it an ideal choice for the user-facing component of this system. Its key features align perfectly with the project's requirements. MyMaps supports the organization of data into distinct layers, allowing for potential future categorization of sightings (e.g., by agency type or date). It enables the customization of map markers with different colors and icons, which can be used to visually encode information. Most importantly, each marker on the map has an associated "info window" that can display detailed information, including formatted text, images, and videos, which is essential for providing context for each reported sighting.  
Internally, every layer in a Google My Map is supported by a structured data table. This table can be viewed and edited directly within the MyMaps user interface, resembling a simple spreadsheet. The central objective of the automated backend pipeline is to generate a data file that perfectly mirrors the structure of this table, ensuring a seamless and error-free import process.

### **5.2. Defining the Data Schema for Import**

Google MyMaps supports the import of data from several file formats, including KML, XLSX, and Google Sheets. For the purpose of this automated workflow, the Comma-Separated Values (CSV) format is the most suitable due to its simplicity, text-based nature, and ease of generation from the Python script.  
To ensure a successful import, the CSV file must adhere to a specific schema. At a minimum, it requires columns that define the location of each point and a title for the corresponding map pin. The location can be provided as separate Latitude and Longitude columns or as a single column containing a full address. For this project, using explicit latitude and longitude coordinates obtained from the Geocoding API is the most reliable method.  
A significant feature of the MyMaps import process is its ability to recognize and import any additional columns present in the CSV file as custom data fields. These custom fields are automatically added to the layer's data table and are displayed in the info window of the corresponding map pin. This mechanism is how the system will deliver the rich, contextual information for each sighting. The following table defines the precise schema for the map\_data.csv file that the Python script will generate.

| Column Header | Data Type | Required? | Description & Formatting Notes | Example |
| :---- | :---- | :---- | :---- | :---- |
| Title | String | Yes | A concise title for the map pin. Should include the type of agency and the primary location (e.g., cross-streets). | ICE Sighting: Clark & Division |
| Latitude | Float | Yes | The WGS 84 latitude coordinate. | 41.9038 |
| Longitude | Float | Yes | The WGS 84 longitude coordinate. | \-87.6318 |
| Timestamp | String (ISO 8601\) | No | The UTC timestamp of the Reddit post. Formatted as YYYY-MM-DDTHH:MM:SSZ. | 2023-10-27T18:30:00Z |
| Description | String | No | A summary of the sighting. Can include excerpts from the post. Max length constraints should be considered. | User reported 3 CBP vehicles parked at the intersection. |
| SourceURL | String (URL) | No | A direct permalink to the Reddit post where the information was sourced. | https://www.reddit.com/r/chicago/comments/... |
| VideoURL | String (URL) | No | If a video link is present in the post, this field will contain the direct URL to the video. | https://www.youtube.com/watch?v=... |
| Agency | String | No | The specific agency identified (ICE, CBP, Police, Border Patrol). Useful for styling/filtering. | ICE |

### **5.3. Handling Rich Media: Images and Videos**

A key requirement for the project is the ability to link to videos and other media from within the map's info windows. It is important to understand the technical constraints of the MyMaps import process in this regard. It is not possible to directly embed video files or even image files via a CSV import; the process is designed for textual and location-based data only.  
The most effective and reliable workaround is to leverage hyperlinks. The Description field within the info window can render basic HTML, including clickable \<a\> tags. The automated workflow should be designed to parse the Reddit post for any links to external media, such as YouTube or other video platforms. When a link is found, its URL should be stored in the dedicated VideoURL column of the CSV. The Python script can then construct a simple HTML hyperlink in the Description field (e.g., \<a href="" target="\_blank"\>View Video Evidence\</a\>) that will appear as a clickable link in the final info window. While MyMaps does have a native feature for embedding YouTube videos when they are added manually through the user interface, this functionality cannot be triggered via a CSV import. Therefore, a standard hyperlink is the most robust and cross-platform compatible solution for the automated workflow.

### **5.4. The Manual Update Process: A Practical Walkthrough**

This subsection provides a clear, step-by-step guide for the human-in-the-loop portion of the workflow. This simple and repeatable process allows the user to keep the map synchronized with the latest data generated by the automated backend pipeline.

1. **Download the Data File:** Navigate to the project's GitHub repository in a web browser. Locate and download the most recent version of the map\_data.csv file to the local machine.  
2. **Access the Google My Map:** Open the specific Google My Map that is designated for this project.  
3. **Remove the Existing Data Layer:** In the map's legend panel on the left, find the layer that contains the sightings data. Click on the three-dot menu icon next to the layer's name and select the "Delete this layer" option. Confirm the deletion. This removes all the old data points from the map.  
4. **Import the New Data:** Click the "Add layer" button in the legend panel. This will create a new, empty layer. Click the "Import" link that appears under the new layer's name.  
5. **Upload the CSV File:** In the import dialog, select the map\_data.csv file that was downloaded in the first step.  
6. **Configure the Import:** Google MyMaps will prompt the user to specify which columns contain the location information. Check the boxes for the Latitude and Longitude columns. Next, it will ask for the column to use for the title of the map pins; select the Title column from the dropdown menu.  
7. **Finalize:** Click "Finish." Google MyMaps will process the CSV file and plot all the new data points on the map. The layer can then be renamed for clarity (e.g., "Law Enforcement Sightings").

This entire manual process is designed to be completed in less than one minute, providing an efficient method to keep the public-facing map fully up-to-date with the intelligence gathered by the automated pipeline.

## **Section 6: Implementation Roadmap and Strategic Recommendations**

This concluding section synthesizes the preceding analysis into a pragmatic, step-by-step implementation plan. It provides strategic recommendations for ensuring the long-term health and accuracy of the data through robust hygiene practices and error handling. Finally, it outlines a vision for future enhancements that could expand the system's capabilities and transition it towards full automation.

### **6.1. Step-by-Step Implementation Guide**

The development and deployment of the system can be approached in a phased manner to ensure a structured and manageable process. The following chronological checklist outlines the key milestones.

1. **Phase 1: Foundational Setup & Configuration (Day 1-2):**  
   * Create all necessary accounts: GitHub, Replit, n8n (Cloud or self-hosted), and Google Cloud Platform.  
   * Within Reddit, create a new "script" type application to obtain the Client ID and Client Secret for API access.  
   * Within the Google Cloud Platform console, create a new project, enable the Geocoding API, and generate a restricted API key.  
   * Initialize the GitHub repository and establish the basic project structure (e.g., folders for code, data, nlp\_model).  
2. **Phase 2: Ingestion Workflow Development (Day 3-4):**  
   * In n8n, create a new workflow. Add a Schedule node and configure it to run at a suitable interval (e.g., every 30 minutes for initial testing).  
   * Add and configure the Reddit node, using the credentials created in Phase 1\. Set up the Search Posts operation with the target subreddits and initial keyword list.  
   * Execute the workflow manually to test the connection and verify that data is being successfully retrieved from Reddit.  
3. **Phase 3: NLP and API Development (Day 5-10):**  
   * Set up the Replit environment by importing the GitHub repository.  
   * Begin the data annotation process for the custom spaCy NER model. This is the most time-intensive manual task. Aim for an initial set of 100-200 annotated examples.  
   * Train the first version of the custom NER model using the spaCy CLI within Replit.  
   * Develop the basic Flask API endpoint in Python to receive data from n8n, process it with the trained spaCy model, and return the extracted entities.  
4. **Phase 4: End-to-End Integration (Day 11-14):**  
   * Modify the n8n workflow to send an HTTP request to the Replit API endpoint for each relevant Reddit post.  
   * In the Python script, implement the call to the Google Geocoding API to convert extracted location strings to coordinates.  
   * Implement the logic to read the existing map\_data.csv from GitHub, append the new, processed data row, and commit the updated file back to the repository.  
   * Implement the deduplication logic described in the following section.  
5. **Phase 5: Visualization, Testing, and Refinement (Day 15+):**  
   * Perform the first full, manual import of the map\_data.csv file into a new Google My Map.  
   * Conduct end-to-end testing of the entire pipeline, from a new Reddit post to a new pin on the map.  
   * Analyze the results and identify areas for improvement. This will likely involve refining the keyword list in n8n and, most importantly, annotating more data to improve the accuracy of the custom NER model.

### **6.2. Data Hygiene: Implementing a Deduplication Strategy**

To ensure the final map is accurate and not cluttered with redundant information, a data deduplication strategy is essential. It is common for multiple users on Reddit to post about the same public event, and the system must be able to identify and filter out these duplicate reports. A simple yet effective deduplication logic should be implemented within the Python script before a new record is written to the CSV file.  
The proposed strategy is based on a temporal and spatial proximity check, drawing from established data hygiene best practices.

1. **Event Caching:** The system will maintain a cache of recently processed events. For simplicity and to keep the system stateless, this cache can be the map\_data.csv file itself. The script will read the existing events from the last 24-48 hours into memory at the start of its execution.  
2. **Similarity Check:** When a new event is successfully extracted and geocoded from a Reddit post, the script will compare it against the events in the recent cache. The comparison will be based on two dimensions: geographic distance and time difference.  
3. **Thresholding:** A set of predefined thresholds will be used to classify an event as a likely duplicate. For example, if a new event's coordinates are within 200 meters of an existing event's coordinates, AND its timestamp is within 60 minutes of the existing event's timestamp, it will be flagged as a duplicate.  
4. **Action:** If an event is flagged as a duplicate, it will be discarded and not added to the CSV file. This prevents multiple pins from appearing on the map for a single real-world incident.

This automated deduplication step will significantly enhance the quality and reliability of the final intelligence product.

### **6.3. Error Handling and System Monitoring**

A production-grade system must be resilient to failure. Robust error handling and monitoring should be implemented at each stage of the pipeline.

* **n8n Workflow:** The n8n workflow should include error handling branches. For transient API errors from Reddit (e.g., 5xx server errors), a retry mechanism with an exponential backoff should be implemented. For permanent client errors (e.g., 4xx authentication errors or 429 rate-limiting errors), the workflow should halt processing for that post and send a notification (e.g., via email or Slack) to the system administrator.  
* **Python Script (Replit):** The Python script must be wrapped in try...except blocks to gracefully handle potential failures. This includes cases where the custom NER model fails to extract any location, the Google Geocoding API returns no valid results for a given string, or there are issues writing the file back to GitHub. These events should be logged for later analysis but should not cause the entire application to crash.  
* **Google Geocoding API:** API usage and any reported errors should be monitored from the Google Cloud Platform console. This is crucial for managing costs and ensuring the service remains within any defined budget or free tier limits.

### **6.4. Future Enhancements**

Once the core system is stable and operational, several enhancements could be implemented to increase its capabilities and value.

* **Transition to Full Automation with Maps JavaScript API:** The most significant future enhancement would be to migrate the visualization layer from Google MyMaps to a custom web application built with the Google Maps JavaScript API. This would eliminate the "manual air gap" and allow the n8n workflow to update the map in real-time by sending data directly to a backend API that manages the map data. This would transform the system into a fully automated, dynamic intelligence platform.  
* **Sentiment and Topic Analysis:** The NLP pipeline could be expanded. After extracting entities, an additional NLP model could be applied to perform sentiment analysis on the Reddit post, classifying it as positive, negative, or neutral. This sentiment score could then be used to dynamically style the map pins (e.g., using different colors), providing an at-a-glance understanding of the context of each sighting.  
* **Expansion of Data Sources:** The ingestion module could be expanded beyond Reddit. The n8n workflow could be enhanced to pull data from other sources, such as monitoring specific Twitter accounts, scraping local news websites via their RSS feeds, or even integrating with public police scanner audio stream aggregators. This would create a more comprehensive and multi-faceted view of events across the city.

#### **Works cited**

1\. Meet Jules Tools: A Command Line Companion for Google's Async Coding Agent, https://developers.googleblog.com/en/meet-jules-tools-a-command-line-companion-for-googles-async-coding-agent/ 2\. Build with Jules, your asynchronous coding agent \- Google Blog, https://blog.google/technology/google-labs/jules/ 3\. AI is transforming how software engineers do their jobs. Just don't call it 'vibe-coding', https://apnews.com/article/ai-vibe-coding-anthropic-assistants-09f35ccc7545ac92447a19565322f13d 4\. Google AI Studio, https://aistudio.google.com/ 5\. Access data of an map created on google my map through API \- Stack Overflow, https://stackoverflow.com/questions/39981796/access-data-of-an-map-created-on-google-my-map-through-api 6\. Possible to Interact with a "My Map" via Google Maps API?, https://support.google.com/maps/thread/222972326/possible-to-interact-with-a-my-map-via-google-maps-api?hl=en 7\. Is it possible to automate my map creation with Google Apps Script?, https://groups.google.com/g/google-apps-script-community/c/z4wNeHO0i3w 8\. How to Export Google My Maps Data Table to Excel \- bpwebs.com, https://www.bpwebs.com/how-to-export-google-my-maps-data-table-to-excel/ 9\. Import map features from a file \- Computer \- My Maps Help \- Google Help, https://support.google.com/mymaps/answer/3024836?hl=en\&co=GENIE.Platform%3DDesktop 10\. Create or open a map \- Computer \- My Maps Help \- Google Help, https://support.google.com/mymaps/answer/3024454?hl=en\&co=GENIE.Platform%3DDesktop 11\. Visualize your data on a custom map using Google My Maps, https://www.google.com/earth/outreach/learn/visualize-your-data-on-a-custom-map-using-google-my-maps/ 12\. Overview | Maps JavaScript API \- Google for Developers, https://developers.google.com/maps/documentation/javascript/overview 13\. Google Maps Platform Documentation | Maps JavaScript API | Google for Developers, https://developers.google.com/maps/documentation/javascript 14\. n8n vs node-red. Which to use | by Daniel Payne \- Medium, https://daniel-payne-keldan-systems.medium.com/n8n-vs-node-red-485e8382b971 15\. n8n vs Node-RED 2025: What Matters Most (and Who Each Is For) \- YouTube, https://www.youtube.com/watch?v=755C5CLN\_84 16\. Reddit integrations | Workflow automation with n8n, https://n8n.io/integrations/reddit/ 17\. Reddit node documentation | n8n Docs, https://docs.n8n.io/integrations/builtin/app-nodes/n8n-nodes-base.reddit/ 18\. Reddit AI digest | n8n workflow template, https://n8n.io/workflows/1895-reddit-ai-digest/ 19\. Reddit Data API Wiki \- Reddit Help, https://support.reddithelp.com/hc/en-us/articles/16160319875092-Reddit-Data-API-Wiki 20\. Reddit API Limits: Where Data Dreams Meet Rate-Limited Reality | Data365.co, https://data365.co/blog/reddit-api-limits 21\. Reddit Advertising API Documentation, https://ads-api.reddit.com/docs/v3/ 22\. node-red-contrib-reddit, https://flows.nodered.org/node/node-red-contrib-reddit 23\. Extract & Filter Reddit Posts & Comments with Keyword Search & Markdown Formatting | n8n workflow template, https://n8n.io/workflows/9201-extract-and-filter-reddit-posts-and-comments-with-keyword-search-and-markdown-formatting/ 24\. What are your thoughts on the police scanner controversy? : r/chicago \- Reddit, https://www.reddit.com/r/chicago/comments/zmj39s/what\_are\_your\_thoughts\_on\_the\_police\_scanner/ 25\. Chicago \- Reddit, https://www.reddit.com/r/chicago/ 26\. What happened to the r\\Chicago subreddit? Seems… different? : r/AskChicago, https://www.reddit.com/r/AskChicago/comments/1m794ny/what\_happened\_to\_the\_rchicago\_subreddit\_seems/ 27\. Using Github and Replit \- Python \- Data to Insight, https://www.datatoinsight.org/using-github 28\. Import from GitHub \- Replit Docs, https://docs.replit.com/replit-workspace/using-git-on-replit/import-repository 29\. Extracting locations from text using Python \- Tutorials Point, https://www.tutorialspoint.com/extracting-locations-from-text-using-python 30\. NLP Libraries in Python \- GeeksforGeeks, https://www.geeksforgeeks.org/nlp/nlp-libraries-in-python/ 31\. Understanding Named Entity Recognition in NLP | CodeSignal Learn, https://codesignal.com/learn/courses/collecting-and-preparing-textual-data-for-classification/lessons/understanding-named-entity-recognition-in-nlp 32\. NLP | Location Tags Extraction \- GeeksforGeeks, https://www.geeksforgeeks.org/nlp/nlp-location-tags-extraction/ 33\. spaCy 101: Everything you need to know, https://spacy.io/usage/spacy-101 34\. Linguistic Features · spaCy Usage Documentation, https://spacy.io/usage/linguistic-features 35\. From Text to Map: Combing Named Entity Recognition and Geographic Information Systems, https://journal.code4lib.org/articles/15405 36\. Extracting and Identifying locations with NLP \+ Spacy \- Stack Overflow, https://stackoverflow.com/questions/77951208/extracting-and-identifying-locations-with-nlp-spacy 37\. Address parsing using spaCy \- Data Science Stack Exchange, https://datascience.stackexchange.com/questions/73783/address-parsing-using-spacy 38\. Automating Address Extraction from Legal Documents with spaCy NER | by Bhanu Prakash Putta | Medium, https://medium.com/@bprakashputta/automating-address-extraction-from-legal-documents-with-spacy-ner-9b5193e57bd0 39\. Custom Named Entity Recognition using spaCy v3 \- Analytics Vidhya, https://www.analyticsvidhya.com/blog/2022/06/custom-named-entity-recognition-using-spacy-v3/ 40\. Building an address parser with spaCy | Globant \- Medium, https://medium.com/globant/building-an-address-parser-with-spacy-e3376b7cff 41\. Geocoding API overview \- Google for Developers, https://developers.google.com/maps/documentation/geocoding/overview 42\. Reverse geocoding (address lookup) request and response \- Google for Developers, https://developers.google.com/maps/documentation/geocoding/requests-reverse-geocoding 43\. Manage API keys | Authentication \- Google Cloud, https://cloud.google.com/docs/authentication/api-keys 44\. Getting started with Google Maps Platform, https://developers.google.com/maps/get-started 45\. Use map layers \- Computer \- My Maps Help \- Google Help, https://support.google.com/mymaps/answer/3024933?hl=en\&co=GENIE.Platform%3DDesktop 46\. My Maps – About \- Google, https://www.google.com/maps/about/mymaps/ 47\. Importing Data from .csv \- Map Maker, https://maps.co/help/layers/importing-csv 48\. How to embed a YT video in Google Earth so that it can be downloaded as KML and work in My Maps, https://support.google.com/earth/thread/201426743/how-to-embed-a-yt-video-in-google-earth-so-that-it-can-be-downloaded-as-kml-and-work-in-my-maps?hl=en 49\. Adding Images and Videos to a Google My Map \- YouTube, https://www.youtube.com/watch?v=Zmo-OcuViUg 50\. Google My Maps: Embed YouTube video not working on iOS only : r/GoogleMaps \- Reddit, https://www.reddit.com/r/GoogleMaps/comments/1czdhtc/google\_my\_maps\_embed\_youtube\_video\_not\_working\_on/ 51\. What Is CRM Data Deduplication? (Tips & Tools) \- Scratchpad, https://www.scratchpad.com/blog/crm-data-deduplication 52\. Deduplication Best Practices \- Insycle, https://support.insycle.com/hc/en-us/articles/6584810088855-Deduplication-Best-Practices 53\. Practical Tips and Strategies for Managing Data Duplication in Large Datasets \- CloudThat, https://www.cloudthat.com/resources/blog/practical-tips-and-strategies-for-managing-data-duplication-in-large-datasets 54\. Maps API \- Google Maps Platform, https://mapsplatform.google.com/lp/maps-apis/ 55\. Chicago police now have 13 scanner feeds online | RadioReference.com Forums, https://forums.radioreference.com/threads/chicago-police-now-have-13-scanner-feeds-online.438847/ 56\. CPD Zone 4 \- Districts 1 and 18 Live Audio | CrimeIsDown.com, https://crimeisdown.com/audio/live/zone4